---
title: K-means, Hiarchical-clustering 알고리즘
category: Basic Machine Learning Algorithm
tag: Clustering
---

**k-means** 알고리즘 그리고 **Hiarchical-clustering** 알고리즘은 Unsupervised Learning의 한 종류로 데이터들을 여러 개의 클러스터로 나눌 때(Clustering) 사용되는 기본적인 Machine Learning 알고리즘 입니다. 이 알고리즘을 이해하기 위해서는  (1) 벡터 사이의 거리 개념 (2) 클러스터 사이의 거리에 대한 개념이 필요합니다. 이 글에서는 일단 (1), (2)에 대해 다룬 후 (3) 알고리즘의 작동 방식 (4) 나눈 클러스터에 대한 평가에 대해 다루겠습니다.

## 벡터, 클러스터 사이의 거리

### 벡터 사이의 거리

두 벡터 $$\textbf{x}=(x_{1},...,x_{p}), \textbf{y}=(y_{1},...,y_{p})$$가 있다고 할 때, 둘 사이의 거리를 측정할 수 있는 다양한 방식이 있다.

* Euclidean distance : $$d(x,y)=(\sum_{i=1}^{p} (x_{i}-y_{i})^{2})^{\frac{1}{2}} $$
* Manhattan distance : $$d(x,y)=\sum_{i=1}^{p} \lvert x_{i}-y_{i} \rvert$$
* Minkowski distance : $$d(x,y)=(\sum_{i=1}^{p} (x_{i}-y_{i})^{m})^{\frac{1}{m}}$$
* cosine similarity : $$\frac{\sum_{i=1}^{n} x_{i} \times y_{i}}{\sqrt{\sum_{i=1}^{n} x_{i}^{2}} \times \sqrt{\sum_{i=1}^{n} y_{i}^{2}}}$$

4 코사인 유사도는 거리(얼마나 먼지)가 아니라 유사도(얼마나 가까운지)에 대한 측정이기 때문에 높을 수록 거리가 가깝다고 생각해야 합니다. 나머지는 거리이기 때문에 값이 낮을 수록 가깝다고 생각해야 합니다.

### 클러스터 사이의 거리

두 클러스터 $$U=\{u_{1},...,u_{n}\}, V=\{v_{1},...,v_{m}\}$$이 있고, 둘 사이의 거리를 $$D(U, V)$$라고 하자. 또한, 두 벡터 $$x, y$$사이의 거리 측정 function을 $$d(x,y)$$라 한다면, 여러 가지 기준으로 클러스터 사이의 거리를 구할 수 있습니다.

* 최단 연결법(Single Linkage Method) : $$D(U,V)=\min d(u_{i}, v_{j})$$, $$i=1,...,n; j=1,...,m$$
* 최장 연결법(Complete Linkage Method) : $$D(U,V)=\max d(u_{i}, v_{j})$$, $$i=1,...,n; j=1,...,m$$
* 평균 연결법(Average Linkage Method) : $$D(U,V)=\frac{\sum_{i=1}^{n} \sum_{j=1}^{m} d(u_{i}, v_{j})}{n \times m}$$
* 중심 연결법(Centroid Linkage Method) : $$D(U,V)=\lvert \lvert \frac{\sum_{i=1}^{n} u_{i}}{n} - \frac{\sum_{j=1}^{m} v_{j}}{m} \lvert \lvert$$

## 알고리즘 작동 방식

### k-means 알고리즘

n개의 데이터가 주어졌을 때, k-means 알고리즘의 목표는 n개의 데이터들이 속한 각 집합 내에서 데이터들과 중심점 사이의 거리를 최소화하는 k개의 집합 $$\textbf{S}=\{S_{1},...,S_{k}\}$$으로 분할하는 것 입니다. 수식적으로 보면 $$\mu_{i}$$를 집합 $$S_{i}$$의 중심점이라고 할 때

> $$ argmin_{\textbf{S}} \sum_{i=1}^{k} \sum_{x \in S_{i}} d(x, \mu_{i}) $$

즉, 중심점 ~ 집합 내 데이터간 거리를 최소화하도록 데이터를 나누는 것입니다. 보통 벡터들 사이의 거리는 유클리드 거리를 사용합니다. 알고리즘의 진행은 아래와 같이 됩니다.

1.  데이터 내에 임의로 k개의 클러스터 중심점(Centroid)을 설정합니다.
2.  모든 데이터에 대해 각 클러스터 중심점까지의 거리 계산합니다.
3.  모든 데이터를 가장 가까운 클러스터 중심점에 속한 클러스터으로 할당
4.  각 클러스터의 중심점을 재설정합니다.
5.  클러스터의 중심점이 수렴할 때까지 1~4를 반복합니다.

이를 그림으로 보면 아래와 같이 됩니다.

<center><a href="https://imgur.com/NuphKOV"><img src="https://i.imgur.com/NuphKOV.png" width="500px" height="500px" title="source: imgur.com" /></a></center>

(a)에서 (i)로 가까워 질수록 클러스터내에서의 그 중심점과 데이터 사이의 거리는 낮아집니다.

### Hiarchical-clustering 알고리즘

k-means 알고리즘이 계속해서 데이터의 클러스터을 재분배한다면, Hiarchical-clustering 알고리즘 중 bottom-up 알고리즘은 모드 데이터를 자기 자신 클러스터으로 하여 둘 사이의 거리가 가장 작은 클러스터들을 묶으면서 하나의 클러스터으로 데이터들을 모두 묶을 때까지 진행합니다. 구체적인 알고리즘의 진행은 아래와 같습니다.

1.  모든 데이터를 개별적인 클러스터 $$(C_{1},...,C_{n})$$에 속한다고 하고 두 개의 클러스터 사이의 거리를 구한다. ($$d_{C_{i}C_{j}}=d_{ij}$$) 그리고 $$t=1$$이라고 하자.
2.  $$d_{ij}$$가 최소인 두 개의 클러스터를 묶고 새로운 클러스터 $$C_{n+t}$$로 둔다. 그리고 $$C_{i},C_{j}$$를 지운다.
3.  클러스터의 집합은 $$(C_{1},...,C_{i-1}, C_{i+1}, ..., C_{j-1}, C_{j+1},...C_{n}, C_{n+t})$$이 되고 다시 이들 사이의 거리를 구한다. 그리고 t=t+1로 둔다.
4. 2과정으로 돌아가서 하나의 클러스터만 남을 때까지 한다.

간단한 위와 같은 데이터를 가지고 위와 같은 과정을 반복해 보겠습니다. 이 예시에서는 클러스터 사이의 거리는 최단 연결법을 사용합니다.

<center><a href="https://imgur.com/IZkOC1b"><img src="https://i.imgur.com/IZkOC1b.png" width="500px" height="400px" title="source: imgur.com" /></a></center>

* A, B, C, D : A와 B가 거리가 2로 제일 짧아서 합쳐집니다.

* (A,B), C, D : C와 D가 거리가 3으로 제일 짧아서 합쳐 집니다.

* (A,B), (C,D) 거리가 6으로 합쳐 집니다.

* (A,B,C,D) 종료

시각화 해보면 아래와 같은 그림이 나오는 데, 이를 dendrogram이라고 부릅니다.

<center><a href="https://imgur.com/2jEcJjP"><img src="https://i.imgur.com/2jEcJjP.png" width="500px" height="400px" title="source: imgur.com" /></a></center>

최종적으로 하나의 클러스터가 나올 때까지 알고리즘이 반복된다는 점에서 적당한 클러스터 개수를 정하고 그에 맞게 잘라내는 과정이 필요합니다. 위의 경우에 클러스터의 개수를 2개로 하고 싶다면, 두 번째 iteration까지 돌리고 난 후의 결과인 (A,B),(C,D)로 결과를 내면 됩니다.

### 클러스터 개수(k)에 대한 선택

클러스터 개수는 몇 개로 나뉠지 알 수 없지만, 사용자는 k의 개수를 지정해 주어야 합니다. k-means, Hiarchical clustring에서는 k의 개수를 정하기 위해서 보통 2가지 방법 정도를 사용합니다.

* 경험적 방법 : 데이터의 개수를 $$n$$개라고 할 때, $$\sqrt{n}$$로 k 개수를 정하는 방법
* Elbow point : 클러스터 개수를 계속해서 늘려가면 각 클러스터 내부거리의 합은 당연히 줄어들 수 밖에 없습니다. k=n이라면 내부 거리는 0이 되는 것을 보면 알 수 있습니다. 그래서 각 클러스터 내부거리의 합이 급격하게 떨어지는 지점을 찾아서 그 지점을 k로 사용합니다.

## Clustering에 대한 평가

Unsupervised Learning인 클러스터링을 평가하는 일은 쉽지 않습니다. 만약 사전에 데이터들의 label이 알려져 있다면, Supervised Learning과 유사한 방법들로 성능을 평가할 수 있지만, 그렇지 않다면 자체적인 기준으로 클러스터링을 평가해야 합니다. 그렇기 때문에 이 평가기준이 괜찮다고 하더라도 정말로 괜찮은 지는 장담할 수는 없습니다. 기본적으로 아래서 제시될 평가 기준들은 클러스터 내에서 데이터들 사이의 거리를 최소화하고 클러스터 사이의 거리는 최대화 할 수록 좋은 기준이라고 생각합니다.

### Dunn index

클러스터가 $$\{C_{1},C_{2}...,C_{m}\}$$로 나누어져 있다고 하고, 클러스터 내의 데이터들 사이의 최대 거리를 $$\{D_{1},...,D_{m}\}$$이라고 하면

> $$Dunn \ Index = \frac{\min(d(C_{i}, C_{j})){max\{D_{1},...,D_{m}\}}$$

클러스터들 사이의 거리가 클수록, 클러스터 내 데이터 간 거리가 작을수록 클러스터링이 잘됬었다고 생각하면 Dunn Index가 클수록 좋은 클러스터링이라고 할 수 있습니다.

<center><a href="https://imgur.com/Mb3qrft"><img src="https://i.imgur.com/Mb3qrft.png" width="500px" height="300px" title="source: imgur.com" /></a></center>

### Silhouette

Silhouette은 데이터 하나에 대해서 정의되는 값 입니다. 그래서 보통 클러스터 마다 평균적인 Silhouette을 구합니다. $$a(i)$$를 데이터 i로부터 갈은 클러스터 내 모든 다른 데이터들 사이 평균거리(클러스터 내의 응집도), $$b(i)$$를 데이터 i로부터 다른 클로스터 내 데이터들 사이의 평균 거리 중 최소값(클러스터 간 분리도)이라고 하면,

> $$S(i)=\frac{b(i)-a(i)}{\max\{a(i), b(i)\}}$$

<center><a href="https://imgur.com/VLcSnp1"><img src="https://i.imgur.com/VLcSnp1.png" width="500px" height="300px" title="source: imgur.com" /></a></center>

만약 R에서 Silhouette을 그려본다면 아래와 같은 그림을 얻을 수 있습니다.

<center><a href="https://imgur.com/nyGJbRW"><img src="https://i.imgur.com/nyGJbRW.png" width="500px" height="300px" title="source: imgur.com" /></a></center>


## R code

```r
#### k-means implementation
library("cluster")
data(iris)

## Selection of K using Elbow point method
set.seed(123)
withinByK <- sapply(1:6, function(j) kmeans(iris[,-5], j, nstart=10)$tot.withinss)
withinByK <- c(NA, withinByK)
plot(withinByK, type="l", xlab="Number of Clusters",
     ylab="Sum of within distance", main="How to determine K?" )
abline(v=3, col="red", lty='dotted')

## k = 3
kmeans.model <- kmeans(iris[,-5], 3, nstart=10)
# nstart : different set of initial centroid point.

kmeans.model$cluster
kmeans.model$withinss
kmeans.model$tot.withinss
#  cluster : Allocated cluster number
#  centers : Centroid of each cluster
#  withinss : Sum of distance between centroid and data point in each cluster
#  tot.withinss : sum(withinss)
#  betweenss : Distance among clusters
#  size : Number of data point in each cluster
#  iter : Iteration number

#### Hiarchical-clustering implementation

distance <- round(dist(iris[,-5]), 2) # 소숫점 둘째 자리까지만

# Draw dendrogram; We can choose distance measure changing 'method' option
par(mfrow=c(2,2))
plot(h <- hclust(distance, method="single"))
plot(h <- hclust(distance, method="complete"))
plot(h <- hclust(distance, method="average"))
plot(h <- hclust(distance, method="centroid"))
```

<center><a href="https://imgur.com/93oNjK7"><img src="https://i.imgur.com/93oNjK7.png" width="600px" height="400px" title="source: imgur.com" /></a></center>
