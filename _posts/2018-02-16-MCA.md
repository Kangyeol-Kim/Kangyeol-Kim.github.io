---
title: Monte Carlo Approximation
category: Baysian Statistics
tag: Statistics
---

Baysian Statistics에서는 알려지지 않은 모수 $$\theta$$에 대한 분포를 가정합니다. $$\theta$$에 대한 분포 자체는 몇 가지 방법으로 구해지거나 근사해 볼 수 있지만, 우리는 분포의 다른 부분이 궁금할 수도 있다. 즉, 특정 영역 A에 대해 $$P(\theta \in A \lvert y_{1},..,y_{n}),  \lvert \theta_{1}-\theta_{2} \lvert , \theta_{1}/\theta_{2}, \max\{\theta_{1},...,\theta_{n}\}$$ 등이 궁금 할 수도 있습니다. 이러한 값들을 정확히 구해내는 것은 어렵지만  $$\theta$$에 대한 분포에서 값들을 random sampling 할 수 있다면, Monte Carlo method를 통해서 이를 근사적으로 구할 수 있습니다.

## The Monte Carlo method

Poisson-Gamma model의 예시를 생각해 보자. 학사 학위가 없는 여성의 출산율($$Y_{i,1} \lvert \theta_{1}$$)과 학위가 있는 여성의 출산율($$Y_{i,2} \lvert \theta_{2}$$)에 대해 $$\theta_{1},\theta_{2}$$ 의 posterior distribution은 각각:

$$
\begin{align*}
	p(\theta_{1} \lvert \sum_{i=1}^{111} Y_{i,1}=217) = dgamma(\theta_{1}, 219, 112) \\
	p(\theta_{2} \lvert \sum_{i=1}^{44} Y_{i,2}=66) = dgamma(\theta_{2}, 68, 45)
\end{align*}
$$

이다. 여기서 $$P(\theta_{1}>\theta_{2} \lvert \sum Y_{i,1}=217, \sum Y_{i,2}=66)$$를 계산하면:

$$
\begin{align*}
	& P(\theta_{1}>\theta_{2}  \lvert  y_{1,1},...,y_{n_{1},1},y_{1,2},...,y_{n_{2},2}) \\
	&= \int_{0}^{\infty} \int_{0}^{\theta_{1}} p(\theta_{1},\theta_{2} \lvert y_{1,1},...,y_{n_{2},2}) d\theta_{2} d\theta_{1} \\
	&= \int_{0}^{\infty} \int_{0}^{\theta_{1}} dgamma(\theta_{1},219,112) \times dgamma(\theta_{2},68,45) d\theta_{2} d\theta_{1} \\
	&= \frac{112^{219}45^{68}}{\Gamma(219)\Gamma(68)} \int_{0}^{\infty} \int_{0}^{\theta_{1}} \theta_{1}^{218} \theta_{2}^{67} e^{-112\theta_{1}-45\theta_{2}} d\theta_{2} d\theta_{1}
\end{align*}
$$

이러한 적분 계산은 기반 지식이 없으면 다소 복잡해 질 수 있습니다. 대안으로 우리는 Monte Carlo approximation을 생각할 수 있다. $$\theta$$를 관심 있는 모수라고 하고, $$y_{1},...,y_{n}$$를 $$p(y_{1},...,y_{n} \lvert \theta)$$에서 뽑은 샘플들이라고 하자. 또한 $$p(\theta \lvert y_{1},...,y_{n})$$에서 S개의 $$\theta$$를 뽑을 수 있다고 하자. $$\{\theta^{(1)},...,\theta^{(S)}\}$$ 표본들의 경험적 분포(empirical distributions)는 근사적으로 $p(\theta \lvert y_{1},...,y_{n})$이고, S가 증가할 수록 approximation은 정확해 진다. \\
$$The \ law \ of \ large \ numbers$$에 따라 $$\theta^{(1)},...,\theta^{(S)} \overset{i.i.d}{\sim} p(\theta \lvert y_{1},...,y_{n})$$ 하다면:

$$
	\frac{1}{S} \sum_{s=1}^{S} g(\theta^{(s)}) \rightarrow E[g(\theta) \lvert y_{1},...,y_{n}] = \int g(\theta)p(\theta \lvert y_{1},...,y_{n}) d\theta \ as \ S \to \infty
$$

이것은 $$S \to \infty$$인 상황에서 아래와 같은 사실임을 의미한다:


*	$$\bar{\theta}=\sum_{s=1}^{S} \theta^{(s)}/S \rightarrow E[\theta \lvert y_{1},...,y_{n}]$$
*	$$\sum_{s=1}^{S}(\theta^{(s)}-\bar{\theta})^{2}/(S-1) \rightarrow Var[\theta \lvert y_{1},...,y_{n}]$$
*	$$\#(\theta^{(s)} \leq c)/S \rightarrow P(\theta \leq c \lvert y_{1},...,y_{n})$$
*	$$Empirical distribution of \{\theta^{(1)},...,\theta^{(S)}\} \rightarrow p(\theta \lvert y_{1},...,y_{n})$$
*	$$Median of \{\theta^{(1)},...,\theta^{(S)}\} \rightarrow \theta_{1/2}$$
*	$$\alpha$-percentile of $\{\theta^{(1)},...,\theta^{(S)}\} \rightarrow \theta_{\alpha}$$

한편, 이 방법을 쓸 때 생기는 에러인 Monte Carlo error에 대한 논의는 추후에 하겠습니다.

## Posterior inference for arbitrary functions

$$\theta$$에 대한 임의의 계산 가능한 함수 $$g(\theta)$$를 생각하자. 예컨대:

$$
	g(\theta)=log \frac{\theta}{1-\theta} = \gamma
$$

큰 수의 법칙은 만약 $$\{\theta^{(1)},\theta^{(2)},...\}$$를 $$\theta$$에 대한 posterior distribution에서 샘플링 한다면 sample average를 통해서 $\log \frac{\theta^{(s)}}{1-\theta^{(s)}} \to E[log \frac{\theta}{1-\theta} \lvert y_{1},...,y_{n}]$$을 보장 받을 수 있다. 하지만 우리는 이와 다른 값 즉, $$\gamma$$에 대해서 관심을 가질 수도 있는데, 다행히도 Monte Carlo approach를 쓴다면:

$$
\begin{equation}
  \left.\begin{aligned}
  sample \ \theta^{(1)} \sim p(\theta \lvert y_{1},...,y_{n}), \ compute \ \gamma^{(1)}=g(\theta^{(1)}) \\
  sample \ \theta^{(2)} \sim p(\theta \lvert y_{1},...,y_{n}), \ compute \ \gamma^{(2)}=g(\theta^{(2)}) \\
  \vdots \\
  sample \ \theta^{(S)} \sim p(\theta \lvert y_{1},...,y_{n}), \ compute \ \gamma^{(S)}=g(\theta^{(S)})
  \end{aligned}\right\} = independently
\end{equation}
$$

위와 같은 방법으로 $$\{\gamma^{(1)},...,\gamma^{(S)}\}$$를 얻을 수 있고, 이는 $$p(\gamma \lvert y_{1},...,y_{n})$$로 부터의 S개의 독립적인 샘플이다. $$S \to \infty$$ 일 때 아래와 같은 사실을 알 수 있다:

*	$$\bar{\gamma} = \sum_{s=1}^{S} \gamma^{(s)}/S \to E[\gamma \lvert y_{1},...,y_{n}]$$
*	$$\sum_{s=1}^{S}(\gamma^{(s)-\bar{\gamma}})^{2}/(S-1) \to Var[\gamma \lvert y_{1},...,y_{n}]$$
*	Empirical distribution of $$\{\gamma^{(1),...,\gamma^{(S)}}\} \rightarrow p(\gamma \lvert y_{1},...,y_{n})$$


### Approximation of Two-value Difference

Birthrate 예시에서 두 가지 그룹의 posterior distribution은:

$$
\begin{align*}
	& \{\theta_{1} \lvert y_{1,1},...,y_{n_{1},1}\} \sim gamma(219,112) \ (without \ bachelor's \ degrees) \\
	& \{\theta_{2} \lvert y_{1,2},...,y_{n_{1},2}\} \sim gamma(68,45) \ (with \ bachelor's \ degrees)
\end{align*}
$$

이제 Monte Carlo approximation을 통해서 $$P(\theta_{1}>\theta_{2} \lvert Y_{1,1}=y_{1,1},...,Y_{n_{2},2}=y_{n_{2},2}),P(\theta_{1} / \theta_{2} \lvert Y_{1,1}=y_{1,1},...,Y_{n_{2},2}=y_{n_{2},2})$$를 값을 계산해보자.

$$
\begin{align*}
	sample \ \theta_{1}^{(1)} \sim p(\theta_{1} \lvert \sum_{i=1}^{111} Y_{i,1}=217), \ sample \ \theta_{2}^{(1)} \sim p(\theta_{2} \lvert \sum_{i=1}^{44} Y_{i,2}=66) \\
	sample \ \theta_{1}^{(2)} \sim p(\theta_{1} \lvert \sum_{i=1}^{111} Y_{i,1}=217), \ sample \ \theta_{2}^{(2)} \sim p(\theta_{2} \lvert \sum_{i=1}^{44} Y_{i,2}=66) \\
	\ \ \ \ \ \vdots  \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \  \\
	sample \ \theta_{1}^{(S)} \sim p(\theta_{1} \lvert \sum_{i=1}^{111} Y_{i,1}=217), \ sample \ \theta_{2}^{(S)} \sim p(\theta_{2} \lvert \sum_{i=1}^{44} Y_{i,2}=66)
\end{align*}
$$

수열 $$\{(\theta_{1}^{(1)},\theta_{2}^{(1)}),...,(\theta_{1}^{(S)},\theta_{2}^{(S)})\}$$은 S개의 $$\theta_{1},\theta_{2}$$에 대한 독립적인 샘플이다. 이를 통해 원하는 값을 Monte Carlo approximation 시킬 수 있다. $$\theta_{1}>\theta_{2}$$의 경우 지시 변수 $$I(\theta_{1}>\theta_{2})$$를 도입한 후에 $$\frac{1}{S} \sum_{s=1}^{S} I(\theta_{1}^{(s)}>\theta_{2}^{(s)})$$를 통해 근사값을 구할 수 있다.

또한 수열 $$\{(\theta_{1}^{(1)} / \theta_{2}^{(1)}),...,(\theta_{1}^{(S)} / \theta_{2}^{(S)})\}$$를 통해 같은 방법으로 $$\theta_{1}/\theta_{2}$$을 구할 수 있다.

## Sampling from predictive distributions
새로운 데이터에 대한 predictive distribution은 다음 두 조건을 만족하는 probability distribution이다.

*	known된 값은 조건화 된다.(conditioned on)
*	unknown된 값은 적분한다.(integrated out)

모집단이 Poisson distribution을 따른다고 하자. 데이터 $$\tilde{Y}$$가 모집단에서 샘플링 되고, 우리가 분포의 $$\theta$$값을 알고 있다면:

$$
	Sampling \ model: \ P(\tilde{Y}=\tilde{y} \lvert \theta)=p(\tilde{y} \lvert \theta)=\theta^{\tilde{y}}e^{-\theta}/\tilde{y}!
$$

하지만 현실적으로 우리는 $$\theta$$를 알지 못하므로, predictive distribution은 해당 모수인 $$\theta$$를 적분하는 것으로 만들어 진다.

$$
	Predictive \ model: \
	P(\tilde{Y}=\tilde{y})=\int p(\tilde{y} \lvert \theta)p(\theta) d\theta
$$

$$\theta \sim gamma(a,b)$$인 경우에 위의 predictive distribution($$P(\tilde{Y}=\tilde{y}) \sim NB(a,b)$$)을 따른다. 이와 같이 알려지지 않은 모수에 대해서 적분 되었지만 관측된 데이터에 대해서 조건화하지 않았을 때, **prior predictive distribution** 이라고 한다. 이러한 분포는 $$\theta$$에 대한 prior distribution이 관측값에 대해서 합리적일 때, 유용하다. 데이터 샘플 $$Y_{1},...,Y_{n}$$을 관측하고 난다면, predictive distribution은 다시 아래와 같이 정의된다:

$$
\begin{align*}
	P(\tilde{Y}=\tilde{y} \lvert Y_{1}=y_{1},...,Y_{n}=y_{n}) &= \int p(\tilde{y} \lvert \theta,y_{1},...,y_{n})p(\theta \lvert y_{1},...,y_{n}) d\theta \\
	&= \int p(\tilde{y} \lvert \theta)p(\theta \lvert y_{1},...,y_{n}) d\theta
\end{align*}
$$

이러한 관측 데이터에 대해서도 조건화된 분포를 **posterior predictive distribution** 이라고 부른다. Poisson model에서 gamma prior를 가진다면 posterior predictive distribution은 $$NB(a+\sum y_{i}, b+n)$$ 이되고, 이는 데이터에 따라서 posterior distribution이 달라지는 것을 의미한다.

많은 모델링 상황에서 $$p(\theta \lvert y_{1},...,y_{n})$$이나 $$p(y \lvert \theta)$$은 샘플링이 쉽지만 $$p(\tilde{y} \lvert y_{1},...,y_{n})$$은 샘플링 하기 어렵다. 이 경우 $$p(\tilde{y} \lvert y_{1},...,y_{n})$$를 Monte Carlo 과정을 통해서 샘플링할 수 있다. 즉, $$p(\tilde{y} \lvert y_{1},...,y_{n})=\int p(\tilde{y} \lvert \theta)p(\theta \lvert y_{1},...,y_{n}) d\theta$$이므로 $$p(\tilde{y} \lvert y_{1},...,y_{n})$$은 $$p(\tilde{y} \lvert \theta)$$에 대한 posterior distribution이다. 샘플링을 아래와 같이 한다면:

$$
\begin{align*}
sample \ \theta^{(1)} \sim p(\theta \lvert y_{1},...,y_{n}), \ sample \ \tilde{y}^{(1)} \sim p(\tilde{y} \lvert \theta^{(1)}) \\
sample \ \theta^{(2)} \sim p(\theta \lvert y_{1},...,y_{n}), \ sample \ \tilde{y}^{(2)} \sim p(\tilde{y} \lvert \theta^{(2)}) \\
\ \ \ \ \ \vdots  \ \ \ \ \ \ \ \ \ \ \ \ \ \vdots \ \ \ \ \ \ \ \ \ \  \\
sample \ \theta^{(S)} \sim p(\theta \lvert y_{1},...,y_{n}), \ sample \ \tilde{y}^{(S)} \sim p(\tilde{y} \lvert \theta^{(S)})
\end{align*}
$$

이를 통해서 $$\{(\theta,\tilde{y})^{(1)},...,(\theta,\tilde{y})^{(S)}\}$$의 독립적인 샘플을 $$(\theta, \tilde{Y})$$의 joint posterior distribution으로부터 구할 수 있고, $$\{\tilde{y}^{(1)},...,\tilde{y}^{S}\}$$은 $$\tilde{Y}$$에 대한 **marginal** posterior distribution의 S개의 독립적인 샘플이다.(마지막 $$\theta$$의 모든 구간에 대한 적분으로 $$\theta$$를 없앨 수 있다. 즉, 이 샘플들은 posterior predictive distribution에서 나온다.)
