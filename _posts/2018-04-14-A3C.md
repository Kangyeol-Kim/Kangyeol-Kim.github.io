---
title: Asynchronous Advantage Actor-Critic(A3C)
category: Reinforcement Learning
tag: Reinforcement Learning
---

모든 그림의 출처는 *RLCode와 A3C 쉽고 깊게 이해하기* 에서 가져왔습니다.(<a src="https://www.slideshare.net/WoongwonLee/rlcode-a3c">링크</a>)

### Baseline A3C

* DQN의 장점
  * 게임 화면을 상태 입력으로 받아서 학습
  * 샘플들 사이의 상관관계 문제 해결

* DQN 부족한 점
  * 많은 메모리 사용
  * 느린 학습 속도
  * 불안정한 학습 과정

* Asynchronous Advantage Actor-Critic(A3C) 등장
  * 샘플 사이의 상관관계를 비동기 업데이트로 해결
  * policy gradient 알고리즘 사용
  * 상대적으로 빠른 학습 속도
  * A3C = 비동기 + Actor-Critic
  * Actor-Critic = Reinforce 알고리즘(Monte Carlo Policy gradient)+실시간 학습

### A3C 직관적 이해

<center><a href="https://imgur.com/ERjqC1G"><img src="https://i.imgur.com/ERjqC1G.png" width="500px" height="350px" title="source: imgur.com" /></a></center>

하지만 행동을 실제 행동과 유사하게 만드는 것에 불과합니다. 따라서 정답으로 치는 행동인 $y_{i}$이 좋은지 아닌지를 판단하기 위해서 앞에 $Q_{\pi}(s,a)$를 곱해 줍니다. 이러한 과정을 비동기적으로 한다는 것은 multi-agent ~ multi-environment를 통해서 global한 뉴럴네트워크를 업데이트 시킨다는 것 입니다.

<center><a href="https://imgur.com/AZFd9lC"><img src="https://i.imgur.com/AZFd9lC.png" width="600px" height="350px" title="source: imgur.com" /></a></center>

### Policy and Reinforcement Learning

#### Policy gradient
정책을 업데이트하는 기준으로 목표함수를 세우고 gradient ascent를 통해서 정책을 업데이트 합니다.

* Policy 근사화
  * 매우 High Dimension이라면 각 상태에 대한 정책을 가지는 것은 힘듭니다.
  * 따라서 정책을 함수화 시켜서 $f(state)=action$꼴로 만듭니다.
  * 보통 인공신경망을 통해서 정책을 근사합니다.
  <center><a href="https://imgur.com/FI8jmWw"><img src="https://i.imgur.com/FI8jmWw.png" width="400px" height="300px" title="source: imgur.com" /></a></center>

* 목표함수
  * 정책 기반 강화학습으로 정책을 업데이트할 때마다 어떤 방향으로 업데이트할 지에 대한 기준을 세웁니다.
  * Agent가 정책 $\pi_{\theta}$에 따라서 가게 되는 경로: $\tau=s_{0},a_{0},r_{1},s_{1},a_{1},\cdots,s_{T}$
  * 목표함수 $J(\theta)$는 경로 동안 받을 것이라고 기대하는 보상의 합(경로가 매번 달라지므로)-
  $J(\theta)=E[\sum_{t=0}^{T-1}r_{t+1}\lvert \pi_{\theta}]=E[r_{1}+r_{2}+\cdots r_{T} \lvert \pi_{\theta}]$

* Gradient ascent
  * $\theta'=\theta+\alpha \nabla_{\theta}J(\theta)$

### Reinforce Algorithm

* $J(\theta)=E[\sum_{t=0}^{T-1}r_{t+1}\lvert \pi_{\theta}]=E_{\tau}[r_{1} \lvert \pi_{\theta}]+E_{\tau}[r_{2} \lvert \pi_{\theta}]+E_{\tau}[r_{3} \lvert \pi_{\theta}]+$으로 분리된다.

* 이는 $=\sum_{t=0}^{T-1}P(s_{t},a_{t}\lvert \tau)R(s_{t},a_{t})=\sum_{t=0}^{T-1}P(s_{t},a_{t} \lvert \tau)r_{t+1}$

* 이를 $\theta$에 대해서 미분하면
  * $\nabla_{\theta}J(\theta)=\sum_{t=0}^{T-1}\nabla_{\theta}P(s_{t},a_{t}\lvert \tau)r_{t+1}$
  * $=\sum_{t=0}^{T-1}P(s_{t},a_{t}\lvert \tau)\frac{\nabla_{\theta} P(s_{t},a_{t}\lvert \tau)}{P(s_{t},a_{t}\lvert \tau)}r_{t+1}$
  * $=\sum_{t=0}^{T-1}P(s_{t},a_{t}\lvert \tau)\nabla_{\theta} \log P(s_{t},a_{t}\lvert \tau)r_{t+1}$
  * 기대값을 계산하지 않고 다음을 계속해서 sampling한다: $\sum_{t=0}^{T-1} \nabla_{\theta} \log P(s_{t},a_{t}\lvert \tau)r_{t+1}$
  * $\nabla_{\theta} \log P(s_{t},a_{t}\lvert \tau)=\nabla_{\theta}[\log P(s_{0})+\log \pi_{\theta}(a_{0}\lvert s_{0})+\log P(s_{1} \lvert s_{0},a_{0})+\log \pi_{\theta}(a_{1}\lvert s_{1})+ \cdots +\log P(s_{t}\lvert s_{t-1},a_{t-1})+\log \pi_{\theta}(a_{t} \lvert s_{t})]$
  $\hspace{7.3em}=\nabla_{\theta} \log \pi_{\theta}(a_{0}\lvert s_{0})+\nabla_{\theta} \log \pi_{\theta}(a_{1}\lvert s_{1})+ \cdots + \nabla_{\theta} \log \pi_{\theta}(a_{t}\lvert s_{t})$

* 여기까지 한 이후에 유도 과정은 다음과 같습니다:
  * $\sum_{t=0}^{T-1}r_{t+1}\nabla_{\theta}\log P(s_{t},a_{t} \lvert \tau)$
  $=\sum_{t=0}^{T-1}r_{t+1}[\nabla_{\theta} \log \pi_{\theta}(a_{0}\lvert s_{0})+\nabla_{\theta} \log \pi_{\theta}(a_{1}\lvert s_{1})+ \cdots + \nabla_{\theta} \log \pi_{\theta}(a_{t}\lvert s_{t})]$
  $=\sum_{t=0}^{T-1}r_{t+1}(\sum_{t'=0}^{t}\nabla_{\theta} \log \pi_{\theta}\log \pi_{\theta}(a_{t'} \lvert s_{t'}))$
  $=\sum_{t=0}^{T-1}\nabla_{\theta} \log \pi_{\theta}(a_{t}\lvert s_{t})\sum_{t'=t+1}^{T}r_{t'}$

* 따라서 기대값을 취하면
  * $\nabla_{\theta}J(\theta)=E_{\tau}[\sum_{t=0}^{T-1}\nabla_{\theta} \log \pi_{\theta}(a_{t}\lvert s_{t})\sum_{t'=t+1}^{T}r_{t'}]$
  * Discount factor를 고려하면 $~E_{\tau}[\sum_{t=0}^{T-1}\nabla_{\theta} \log \pi_{\theta}(a_{t}\lvert s_{t})\sum_{t'=t+1}^{T}\gamma^{t'-t-1}r_{t'}]$
  * $\sum_{t'=t+1}^{T}\gamma^{t'-t-1}r_{t'}=G){t}$라고 합니다.
  * 하나의 에피소드를 하나의 샘플로 하고 sampling를 해서 기대값을 구합니다.
  * 이러한 Monte carlo policy gradient를 Reinforce라고 합니다.

* Reinforce의 문제
  * Variance가 높다.
  * 에피소드 끝난 후에 업데이트가 가능하다.

### Actor-Critic

* $\nabla_{\theta}J(\theta)=E_{\tau}[\sum_{t=0}^{T-1}\nabla_{\theta} \log \pi_{\theta}(a_{t}\lvert s_{t})G_{t}]$
$=E_{\tau}[\sum_{t=0}^{T-1}\nabla_{\theta} \log \pi_{\theta}(a_{t}\lvert s_{t})]E_{r_{t+1},s_{t+1},...,r_{T}, s_{T}}[G_{t}]$
* $E_{r_{t+1},s_{t+1},...,r_{T}, s_{T}}[G_{t}]=Q(s_{t},a_{t})$이다..
* $\nabla_{\theta}J(\theta)=E_{\tau}[\sum_{t=0}^{T-1}\nabla_{\theta} \log \pi_{\theta}(a_{t}\lvert s_{t})Q_{\pi_{\theta}}(s_{t},a_{t})]$가 되고,
* 모든 경우에 $Q_{\pi_{\theta}}(s_{t},a_{t})$를 알 수 있는 것은 아니므로 $Q_{\pi_{\theta}}(s_{t},a_{t}) \sim Q_{w}(s_{t}, a_{t})$로 근사한다. 이를 Critic이라고 한다.
* $\nabla J(\theta) \sim \nabla_{\theta} \log \pi_{\theta}(a_{t}\lvert s_{t})Q_{w}(s_{t},a_{t})$에서
  * $Q_{w}(s_{t},a_{t})-baseline$를 해줍니다. 이는 (+10, +3) 인 것을 (+3, -4)로 만들어 주는 것 입니다. Variance를 줄여 줍니다(?)
  * $\nabla J(\theta) \sim \nabla_{\theta} \log \pi_{\theta}(a_{t}\lvert s_{t})(Q_{w}(s_{t},a_{t})-V_{v}(s_{t}))$
* $Q, V$ 둘 다 근사하는 것은 비효율적이다.
  * $Q(s_{t},a_{t})=E[r_{t+1}+\gamma V(s_{t+1}) \lvert s_{t},a_{t}]$이므로
  * $\nabla J(\theta) \sim \nabla_{\theta} \log \pi_{\theta}(a_{t}\lvert s_{t})(r_{t+1}+\gamma V_{v}(s_{t+1})-V_{v}(s_{t}))$로 만들어서 $Q$근사를 축약할 수 있습니다.

* Actor
  * 정책을 근사 : $\theta$
  * $\nabla_{\theta} \log \pi_{\theta}(a_{t}\lvert s_{t})(r_{t+1}+\gamma V_{v}(s_{t+1})-V_{v}(s_{t}))$로 업데이트

* Critic
  * 가치함수를 근사 : $v$
  * $(r_{t+1}+\gamma V_{v}(s_{t+1})-V_{v}(s_{t})^{2}$의 오차함수로 업데이트

  <center><a href="https://imgur.com/GhtTnXY"><img src="https://i.imgur.com/GhtTnXY.png" width="500px" height="350px" title="source: imgur.com" /></a></center>

  <center><a href="https://imgur.com/GhtTnXY"><img src="https://i.imgur.com/GhtTnXY.png" width="500px" height="350px" title="source: imgur.com" /></a></center>

### A3C

Actor-Critic과 다른 점은 Actor를 업데이트하는 과정에서:

* Multi-step loss function : 여러 스텝 간 후에 업데이트
* Entropy loss function : exploration을 가능하게 한다.

#### Multi-step loss function

<center><a href="https://imgur.com/zgp5LuN"><img src="https://i.imgur.com/zgp5LuN.png" width="800px" height="500px" title="source: imgur.com" /></a></center>

#### Entropy loss function

<center><a href="https://imgur.com/aWyZQfh"><img src="https://i.imgur.com/aWyZQfh.png" width="800px" height="500px" title="source: imgur.com" /></a></center>

#### Thread (1)

<center><a href="https://imgur.com/OlQNqAk"><img src="https://i.imgur.com/OlQNqAk.png" width="800px" height="500px" title="source: imgur.com" /></a></center>
