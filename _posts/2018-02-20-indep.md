---
title: 독립/조건부독립 그리고 독립성 검정
category: Some Concepts
tag: Statistics
---

**독립(Independence)과 조건부 독립(Conditional Independence)** 은 통계적으로 매우 중요한 개념입니다. 확률 변수들 사이의 독립성 혹은 조건부 독립이라고 밝혀진다면 계산 과정이 간소화되기 때문입니다. 이 글에서는 독립과 조건부 독립의 정의에 대해서 다루고 연속형과 범주형 변수 사이의 독립성을 검정하는 방법에는 무엇이 있는지 살펴보겠습니다. 참고자료는 wikipedia와 고려대학교 최태련 교수님의 17-2 수리통계특강 자료입니다.

## 독립(Independence)

독립의 수학적 정의는 다음과 같습니다.

**DEFINITION (independence)** 사건 A, B는 아래의 조건 중 하나일 때 independent하다:
$$P(A \cap B)=P(A)P(B)$$ / A, B 둘 중 하나의 확률값이 0 혹은 1 / $$P(A \lvert B)=P(A)$$

두 사건이 독립이라는 것은 하나의 사건 발생이 다른 사건에 영향을 주지 않는 것을 의미합니다. 그래서 사건 두 개가 동시에 발생하는 것의 확률과 각각의 사건 확률을 곱한 것과 같게 됩니다. 또는 사건이 발생할 확률이 0 또는 1이라면 사실 다른 사건과 관계 없이 반드시 발생하거나 발생하지 않기 때문에 두 사건이 독립이 됩니다. 한편, 사건이 두 개 이상 존재할 때와 사건 집합인 실험들 사이의 독립성은 다음과 같이 결정합니다.

**사건(event)이 두 개 이상일 때**

$$
C_{1},...,C_{n}: independent
\Longleftrightarrow
\begin{cases}
  P(C_{i} \cap C_{j})=P(C_{i})P(C_{j}) \\
  P(C_{i} \cap C_{j} \cap C_{k})=P(C_{i})P(C_{j})P(C_{k}) \\
  ...... \\
  P(C_{1} \cap C_{2} \cap  \cdot\cdot\cdot \cap C_{n})=P(C_{1})P(C_{2}) \cdot\cdot\cdot P(C_{n})
\end{cases}
$$

**실험(experiment) 두 개에 대해서**

$$E_{1} \equiv \{C_{11}, C_{12}, \cdots, C_{1m}\}$ \& $E_{2} \equiv \{C_{21}, C_{22}, \cdots, C_{2n} \}$$ : independent \\
$$\Longleftrightarrow C_{1i}, C_{2j}:independent (i=1, \cdots, m; j=1, \cdots, n)$$

## 조건부 독립(Conditional Independence)

조건부 독립은 어떠한 사건이 발생한 상황에서 두 사건의 독립이 되는 경우를 의미합니다.

**DEFINITION(I) (conditional independence)** 사건 F, G는 H가 주어진 상황에서 conditionally independent 하다 만약
> $$P(F \cap G \lvert H)=P(F \lvert H)P(G \lvert H)$$

한편, $$P(F \cap G \lvert H)=P(G \lvert H)P(F \lvert H \cap G)$$ 이므로 아래와 같이 정의 내릴 수도 있다.

**DEFINITION(II) (conditional independence)** 사건 F, G는 H가 주어진 상황에서 conditionally independent 하다. 만약 $$ P(G \lvert H)P(F \lvert H \cap G)=P(F \lvert H)P(G \lvert H)에서 양변에 $$ $$P(G \lvert H)$$를 약분한다면:  
> $$P(F \lvert H \cap G)=P(F \lvert H)$$

두 번째 정의는 사건 H가 주어졌을 때, G의 발생은 F에 영향을 미치지 않습니다. 즉, 사건 G와 F는 서로 상관없는 사건입니다.

## 독립성 검정

데이터 $$\mathcal{D}$$가 주어졌을 때, 설명변수는 연속형 변수(Continuous variable)이거나 범주형 변수(Categorical variable)입니다. 셜명 변수의 형태에 따라서 검정방법이 다릅니다. 즉, 연속 ~ 연속 / 범주 ~ 범주로 나누어서 독립성 검정을 살펴 보겠습니다.

### 연속형 변수 ~ 연속형 변수

연속형 변수의 독립성을 검정하는 것은 어려운 일입니다. 완벽한 방법은 아니지만 여기서는 선형 회귀 분석을 할 때 검정하는 방법을 통해서 연속형 변수들 사이의 관계를 살펴보겠습니다. 선형 회귀 분석에서 연속형 변수들 사이의 다중공산성이 존재할 수 있습니다. 이는 상관계수를 구해 보거나 Variance inflation factor(VIF)을 구해서 확인합니다. VIF는 하나의 변수가 다른 변수에 의해서 얼마나 선형적으로 설명되냐를 나타내 줍니다. 예를 들어 $$1$$번째 변수 $$X_{1}$$에 대해서 먼저

> $$X_{j} = \alpha_{0} + X_{1}=\alpha _{2} X_{2}+\alpha _{3} X_{3}+\cdots +\alpha _{k} X_{k}$$

로 다중 선형 회귀식을 적합합니다. 그리고 $$VIF_{1}=\frac{1}{1-R_{1}^{2}}$$ 로 계산할 수 있습니다. $$R_{1}^{2}$$는 변수 1에 대한 설명계수입니다. 보통 VIF 값이 10을 넘어
가면 즉, 설명계수 값이 0.9보다 크면 이 말은 변수의 90퍼센트가 나머지 변수로 설명된다는 말이므로 그 변수를 회귀 적합에서 제외시키곤 합니다.

```r
# R code
car <-read.csv('car.csv', header = TRUE)
car <- na.omit(car)

# (1) continuous variable ~ continuous variable

library(psych)
con.var <- car[, c(5, 6, 7, 9, 10)]
pairs.panels(con.var)

library(car)
fit <- lm(car[, 1] ~., data = con.var)
vif(fit)
```

### 범주형 변수 ~ 범주형 변수

범주형 변수 사이의 독립성 검정으로 가장 많이 쓰이는 방법은 **카이제곱 검정** 입니다. 예를 들어 다음과 같은 자료가 있다고 하면:

| | 흡연 | 비흡연 | 전체 |
|비만| 10 | 20 | 30 |
|정상체중| 15 | 55 | 70 |
| 전체 | 25 | 75 | 100 |

이 자료에서 비만여부와 흡연여부의 상관성을 검정해 보려고 합니다. 먼저 이렇게 관찰된 값들을 **관측빈도(Observation Number)** 라고 합니다. 전체로 봤을 때, 비만:정상체중 비율은 3:7 입니다. 만약 두 가지 변수가 관련성이 적다면 이러한 경향은 흡연자 집단과 비흡연자 집단에서도 마찬가지 여야 합니다. 즉, 흡연자, 비흡연자 집단에서도 각각 비만:정상체중 비율이 3:7이 되어야 한다는 것 입니다. 흡연자가 25명 이므로 비만:정상체중이 각각 7.5, 17.5명이 되어야 합니다. 이를 **기대빈도(Expected Number)** 라고 합니다. 만약 관측빈도와 기대빈도가 유사하다면 흡연여부와 비만여부가 별다른 관련성이 없다고 할 수 있습니다.

카이제곱 검정은 이를 검정하기 위해서 **귀무가설($$H_{0}$$): 두 변수는 연관성이 없다 VS 대립가설($$H_{1}$$): 두 변수는 연관성이 있다** 로 놓습니다. 관측빈도를 OBS, 기대빈도를 EXP라고 한다면, $$H_{0}$$아래서 $$\frac{(OBS-EXP)^{2}}{EXP}$$의 값들을 계산합니다. $$N \times M$$ 표에서 모든 모든 관측빈도들을 저렇게 계산하면 그 값은 자유도가 $$(N-1) \times (M-1)$$인 $$\chi^{2}$$분포를 따른다고 알려져 있습니다. 이를 요약하면:

> $$\sum_{\forall OBS}\frac{(OBS-EXP)^{2}}{EXP} \sim \chi^{2}((N-1) \times (M-1))$$

위의 예시에서 이를 계산하면, $$\frac{2.5^{2}}{7.5} + \frac{(-2.5)^{2}}{22.5} + \frac{(-2.5)^{2}}{17.5} + \frac{2.5^{2}}{52.5}=1.58$$ 입니다. 자유도가 1인 카이제곱 분포에서 1.58이 관찰될 가능성(p-value)은 0.05보다 큽니다.(=충분히 관찰될 수 있음) 따라서 귀무가설을 기각할 수 없고 두 변수는 연관성이 없다라고 할 수 있습니다.

하지만 카이제곱 검정은 결국 제곱합이라는 점에서 표본 수가 많을 수록 그리고 테이블의 행과 열이 늘어 날수록 그 값이 증가하고 따라서 결과가 왜곡될 수 있습니다. 그에 따라서 카이제곱값을 표준화 시키는 작업이 필요합니다. **Cramér's V** 방법은 카이제곱 값을 표준화 시킨 값으로 $$[0, 1]$$ 사이의 값을 반환합니다. $$n_{ij}$$를 테이블 $$i$$행 $$j$$ 열에 있는 관측값이라고 할 때, 카이제곱 값은 다음과 같은 형태입니다.

$$\chi^{2} = \sum_{i,j} \frac{(n_{ij}-\frac{n_{i.}n_{.j}}{n})^{2}}{\frac{n_{i.}n_{.j}}{n}}$$

Cramér's V는 이 값을 전체 관측 수와 행, 열 중 최소값-1 로 나누어 줍니다.

> $$V = \sqrt{\frac{\chi^{2}/n}{\min(N-1, M-1)}}$$

측정된 $$V$$값으로 두 범주형 변수 사이의 연관성 여부를 판단할 수 있습니다.

```r
# R code
# In succession to the above code, Cramér's V implementation
comb <- t(combn(1:5, 2))
res <- c()
cat.var[, 2] <- as.factor(cat.var[, 2])

for(i in 1:10) {
  tbl <- table(cat.var[, comb[i, 1]], cat.var[, comb[i, 2]])
  chi.t = chisq.test(tbl)
  res[i] <- sqrt((chi.t$statistic / sum(tbl)) / min(nrow(tbl), ncol(tbl)))
}

res
```
