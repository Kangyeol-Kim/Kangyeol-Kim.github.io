---
title: Principal Component Analysis(PCA)
category: Basic Machine Learning Algorithm
tag: Statistics
---

## Orthogonal Matrices

두개의 벡터 $x, y \in R^{n}$이 있을 때 $x^{t}y=0$라면 ***orthogonal*** 하다고 합니다. 또한 $\sqrt{x_{1}^{2}+x_{2}^{2}+\cdots+x_{n}^{2}}=1$이라면 ***normalized*** 되었다고 합니다. 한편 $U \in R^{n \times n}$은 각 열들이 서로 orthogonal하고 normalized되어 있다면 ***orthogonal*** 하다고 불립니다. 즉:

> $U^{t}U = UU^{t}=I_{n} \Rightarrow U^{t}=U^{-1}$

그리고 만약 $A$가 대칭행렬이라면 이를 eigenvector와 eigen value로 분해하는 Spectral Decomposition 과정에서 orthogonal한 eigenvector 행렬 $U$를 가지게 됩니다. $\Lambda=diag(\lambda_{1},...,\lambda_{n})$라고 한다면, $A$는 다음과 같이 분해됩니다.

> $A = U\LambdaU^{t}$

이러한 사실은 PCA에서 제약조건들에서 쓰이게 됩니다.

## Principal Component Analysis(PCA)

Principal Component Analysis(PCA)는 많은 수의 설명변수를 선형 결합을 통해서 다수의 설명변수를 잘 설명 해주는 새로운 변수를 만드는 목적을 가지고 있습니다. 이와 같은 방법은 설명변수의 차원을 축소시켜 준다는 점에서 *dimension reduction* 효과를 기대할 수 있습니다. 그렇다면 어떻게 만들어야 *다수의 설명 변수를 잘 설명 해주는* 것일까요? PCA에서는 데이터의 분산을 최대로 하는 변수를 그렇다고 합니다. 왜냐하면 새로운 변수의 분산이 크다는 것은 데이터가 그 변수를 통해서 잘 구분된다는 의미이기 때문입니다.

<center><a href="https://imgur.com/0wYVE33"><img src="https://i.imgur.com/0wYVE33.png" width="400px" height="400px" title="source: imgur.com" /></a></center>

위의 그림에서 PCA는 파란색 축을 찾으려고 합니다. 이는 분산이 제일 큰 축입니다. 데이터의 산포를 보면 파란 축으로 데이터를 projection 시켰을 때 가장 잘 퍼져(분산이 가장 커) 있습니다. 이는 이 축으로 데이터들을 잘 구분해 낼 수 있는 것을 의미합니다. 따라서 분산은 데이터의 정보량이라고도 할 수 있습니다. 이러한 축을 **Principal Components** 라고 합니다. 설명변수들을 $X_{(n \times p)}$, Principal Components을 $Z$라고 한다면:

$Z_{1}=u_{11}X_{1}+u_{21}X_{2}+\cdots+u_{p1}X_{p}(PC-1)$
$Z_{2}=u_{12}X_{1}+u_{22}X_{2}+\cdots+u_{p2}X_{p}(PC-2)$

와 같은 $Z$들을 찾고자 하는 것 입니다. 따라서 이에 해당하는 $u_{ij}$를 찾아야 합니다. 방금 전에 축에 내적한 값들의 분산을 최대화 시키는 것이 목적이였기 때문에 결국 $z=Xu$의 분산을 maximize하는 $u$를 선택하면 됩니다: $u_{target}=argmax_{u} Var(z)=Var(Xu)=u^{t}Var(X)u$

* X가 standardized 되었을 때, $Corr(X)=R=Var(X)=\frac{1}{n-1}X^{t}X$
* 제약조건 : $u^{t}u=1$

제약조건 아래서 라그랑주 승수법에 따라서

>$L(u, \lambda)=u^{t}Ru-\lambda(u^{t}u-1)$

이제 $u$에 대해 미분하고 이를 0으로 놓으면

> $(R+R^{t})u-2\lambda u=0 \Leftrightarrow Ru=\lambda u$

이를 통해 $u$는 $R$의 eigenvector임을 알 수 있습니다. 또한 $Var(z)=u^{t}Ru=u^{t}\lambda u = \lambda $로 $z$의 분산이 eigenvalue인 것은 명확합니다.

이제 $u$가 $R$에 대한 eigenvector인 것을 알았으니, Spectral Decomposition을 통해서 $R$를 분해하면 모든 $u$값을 구할 수 있습니다. 또한 맨 처음부분에서 대칭행렬의 eigenvector 행렬은 orthogonal하다고 했으니 $R=U \Lambda U^{t}$로 분해되며

* $U=(u_{1},...,u_{p})=(PC-1,..,PC-p)$, orthogonal
* $\Lambda=diag(\lambda_{1},...,\lambda_{p})$, $\lambda_{1} \geq \cdots \geq \lambda_{p}$
* $Var(Xu_{k})=\lambda_{k} \Leftrightarrow Ru_{k}=\lambda_{k}u_{k}$;

이는 PC-1이 분산이 가장 높은($\lambda_{1}$) 방향이며, PC-2가 그 다음으로 높은 분산을 가진 방향이라는 뜻입니다. 사실 $u_{k}$를 구하려면 $u_{1},...,u_{k-1}$에 orthogonal하다는 제약조건 역시 충족시켜야 하기 때문에 분산(정보량)이 차례로 낮아진다고 생각하시면 됩니다. 한편, $R=U\LambdaU^{t} \Leftrightarrow \Lambda = U^{t}RU$이므로 다음과 같은 관계가 있습니다:

>$\lambda_{1}+\cdots+\lambda_{p}=trace(\Lambda)=trace(U^{t}RU)=trace(R)=p$

$\lambda_{1}+\cdots+\lambda_{p}$가 고정되기 때문에 $\lambda_{k}$가 의미하는 것은 전체 분산(정보량) 중 $PC-k$가 설명하는 부분이라고 할 수 있습니다. 또한 $\lambda_{1}+\cdots+\lambda_{k}$는 $PC-k$까지로 설명되는 분산(정보량)이라고 할 수 있습니다. 이를 각각 비율로 나타내면 아래와 같습니다:

> PC-k가 설명하는 부분 : $\frac{\lambda_{k}}{\lambda_{1}+\cdots+\lambda_{p}}$, PC-k까지 설명하는 부분 $\frac{\lambda_{1}+\cdots+\lambda_{k}}{\lambda_{1}+\cdots+\lambda_{p}}$

## 다른 Decomposition 방법

위에서는 eigenvector-eigenvalue를 찾기 위해서 Spectral Decomposition을 사용해서 $R=U\Lambda U^{t}$와 같이 분해하였습니다. 하지만 Singular Value Decomposition(SVD)을 통해서도 $R=\frac{1}{n-1}X^{t}X$의 eigenvector를 구할 수 있습니다.

### Singular Value Decomposition

$X_{n \times p}$가 주어졌을 때, 이를 $X=U D V^{t}$로 분해할 수 있습니다.

* $U \in R^{n \times n}$, $U U^{t}=I_{n}$; $XX^{t}$의 orthogonal eigenvectors
* $D \in R^{n \times p}$, 직사대각행렬입니다.
* $V \in R^{p \times p}$, $V V^{t}=I_{p}$; $X^{t}X$의 orthogonal eigenvectors

$D$의 대각성분은 $\min(rank(X))$개의 성분을 가집니다. 이러한 분해의 의미를 기하학적으로 본다면 다음과 같습니다:

<center><a href="https://imgur.com/TzHuv4b"><img src="https://i.imgur.com/TzHuv4b.png" width="400px" height="400px" title="source: imgur.com" /></a></center>

이는 rotation과 그쪽 방향으로 다시 늘리고 다시 한번 rotation 시키는 것 입니다. 한편, 이러한 분해를 $R$에 대입하면:

>$R = \frac{1}{n-1}X^{t}X = \frac{1}{n-1}V D^{t} U^{t} U D V^{t}=V \frac{1}{n-1} diag(d_{j}^{2}) V^{t}$

위에서 했던 분해와 유사하게 Principal Component들로 $V$라는 오른쪽 Singular Vector들을 얻을 수 있고, 그에 따른 설명량으로 $\frac{1}{n-1}d_{j}^{2}$인 Singular Value들을 얻을 수 있습니다.
