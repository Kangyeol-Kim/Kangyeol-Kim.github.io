---
title: Neural Network 시작하기
category: Deep Learning
tag: Deep Learning
---

## Backpropagation

Neural Net은 computational graph 꼴을 가집니다.

<<그림>>

모델을 이러한 구조로 나타낸다면 *Backpropagation* 방법을 통해서 각 node의 gradient를 손쉽게 계산할 수 있습니다. 하나의 node에 대해 우리가 아는 것은 지역적으로 어떠한 값이 그 node에 연결되어 있는가 입니다. 연결되어 있는 값에 대해서 local gradient를 계산할 수 있습니다.

<center><a href="https://imgur.com/HwE4e47"><img src="https://i.imgur.com/HwE4e47.png" width="400px" height="400px" title="source: imgur.com" /></a></center>

(이미지 출처: cs231 Lecture note 4)

그래프의 끝까지 도달하고 나면 마지막 단에서 계산된 gradient가 돌아오면서 미리 계산된 gradient값들과 곱해지면서 더 전의 input 값에 대한 gradient 값을 구할 수 있습니다. 정리하면 한 노드를 기준으로 나가는 값에서 구해지는 upstream gradient와 들어오는 값을 통해 구해지는 local gradient의 곱이 다시 뒤로 전달되는 것 입니다.  다이런 과정을 통해 계속 진행해 진다면 우리는 처음 들어오는 가중치의 gradient 값까지 구할 수 있을 것 입니다. 이런 식의 local gradient만 계속해서 계산해내면 되기 때문에 효율적입니다. 이렇게 가중치의 gradient를 계산하는 방법을 Backpropagation이라고 합니다.

* 많이 사용되는 computational graph부분을 수학적으로 미리 계산해서 사용하기도 합니다. 예를 들어 Sigmoid 함수를 computational graph로 나타내면 다음과 같이 그려집니다:

> $(\times -1) \rightarrow \exp \rightarrow +1 \rightarrow \frac{1}{x}$

하지만 사실 $\sigma(x)=\frac{1}{1+e^{-x}}$에 대해서 손 쉽게 미분계산을 할 수 있습니다.

> $\frac{\partial \sigma(x)}{\partial x}=\sigma(x) \cdot (1-\sigma(x))$

이를 Sigmoid gate라고 하고 위와 같은 사실을 이용하면 계산을 더욱 빠르게 할 수 있습니다.

* 만약 node가 두 개 이상의 node에 연결되어 진다면 gradient는 더해집니다.

<center><a href="https://imgur.com/wb90jjw"><img src="https://i.imgur.com/wb90jjw.png" width="400px" height="300px" title="source: imgur.com" /></a></center>
(이미지 출처: cs231 Lecture note 4)

### Gradient for vectorized

위에서는 scalar로 gradient를 계산했지만 사실 output 부분은 벡터입니다. 그리고 input 역시 벡터라면 미분을 하면 $Jacobian \ Matrix_{output-d \times input-d}$로 행렬꼴의 gradient를 가지게 됩니다. 참고로 벡터 사이의 미분 관계는 다음과 같습니다.

$$
y=\begin{pmatrix}
  y_{1} \\
  y_{2} \\
  \vdots \\
  y_{m}
\end{pmatrix},
x=\begin{pmatrix}
  x_{1} \\
  x_{2} \\
  \vdots \\
  x_{n}
\end{pmatrix} \Rightarrow
\frac{\partial y}{\partial x}=\begin{pmatrix}
  \frac{y_{1}}{x_{1}} & \frac{y_{1}}{x_{2}} & \cdots & \frac{y_{1}}{x_{n}} \\
  \frac{y_{2}}{x_{1}} & \frac{y_{2}}{x_{2}} & \cdots & \frac{y_{2}}{x_{n}} \\
  \vdots & \vdots & \ddots & \vdots \\
  \frac{y_{m}}{x_{1}} & \frac{y_{m}}{x_{2}} & \cdots & \frac{y_{m}}{x_{n}} \\
\end{pmatrix}
$$

보기에는 계산량이 많아진 것처럼 보이지만 $q=W \dots x, q \in \mathbb{R}^{n}, W \in \mathbb{R}^{n \times n}, x \in \mathbb{R}^{n}$ 꼴에서 계산을 쉽게 요약해 볼 수 있습니다.

>$\frac{\partial q_{k}}{\partial W_{i,j}}=1_{k=i}x_{j}, \frac{\partial q_{k}}{\partial x_{i}}=W_{k,i} $

## Neural Net & Perceptron

Neural Network은 머신 러닝 알고리즘 중 하나인 ANN(Artificial Neural Net)를 보완 발전시켜서 만든 구조입니다. Neural Net은 인간의 뇌 구조 중 뉴런(Neuron)을 본떠서 만든 퍼셉트론(Perceptron)이라는 단위로 구성됩니다.

<center><a href="https://imgur.com/1AvWf1e"><img src="https://i.imgur.com/1AvWf1e.png" width="400px" height="400px" title="source: imgur.com" /></a></center>

왼편 그림에서 알 수 있듯이 인간의 뉴런은 여러 개의 자극을 받아 결과를 냅니다. 인공적인 퍼셉트론 역시 INPUT으로 무언가를 받아서 OUTPUT를 냅니다. 좀 더 구체적으로는 INPUT으로 들어올 때 가중치(weight)가 곱해지고 이를 모두 더한 후에 activation function이라고 불리는 함수를 통과하여 결과를 내게 됩니다. 퍼셉트론 여러개를 이용하면 Neural Net 구조를 만들 수 있습니다. 퍼셉트론을 여러 개 쌓는 것은 선형 모형을 여러개 겹쳐서 쓰는 것을 의미합니다. 또한 activation funciton의 도입은 선형 모델들에 비선형성을 주는 것 입니다. 다음은 아주 간단한 Neural Net 구조입니다.

<center><a href="https://imgur.com/R6MxuaL"><img src="https://i.imgur.com/R6MxuaL.png" width="400px" height="400px" title="source: imgur.com" /></a></center>

Neural Network가 만들어지면 위와 같은 구조를 가지게 됩니다: *input layer-hidden layer-output layer*. 이 떄 hidden layer 개수를 마음대로 정할 수 있는 데, 이를 계속해서 늘려나갈 수 있습니다.  이를 deep 해진다고 표현합니다. Deep Neural Network는 여기서 나온 말 입니다. 이 구조를 요약하면 다음과 같습니다:

- input layer node 수(=데이터 변수 개수) : 3
- hidden layer 개수 : 2
- hidden layer node 수 : 4, 4
- output node 수 : 1

input layer node 수는 데이터에서 변수의 개수를 통해서 결정됩니다. 각 변수의 값이 node안으로 들어가게 되는 것 입니다.  또한 output node 는 데이터의 타겟 변수를 의미합니다. 즉, 위와 같은 Neural Net 구조는 $f_{NN}$를 해당 모델의 함수라고 한다면 $y = f_{NN}(x_{1}, x_{2}, x_{3})$와 같습니다. 한편 각 layer를 연결하는 화살표인 weight에 대해서 살펴봅시다. 각 layer들의 node와 node 간에는 반드시 하나의 weight가 존재합니다. 따라서 input layer - hidden layer 1를 연결하는 weight는 $$3 \times 4 = 12$$개가 존재하고, hidden layer 1 - hidden layer 2를 연결하는 weight는 $$4 \times 4 = 16$$개가 존재합니다. 간단하게 input layer - hidden layer 1 사이를 계산하여 hidden layer 1의 node 값들을 계산해 봅시다.$x_{1}, x_{2}, x_{3}$를 세 개의 변수값이라고 하고 $$w_{ij}, \ i=1,2,3; \ j=1,2,3,4$$를 input layer 하나의 node와 hidden layer 1 하나의 node 사이의 weight라고 하고 $$f$$를 활성함수라고 한다면

- hidden layer 1 - node1 = $$f(\sum_{i=1}^{3} w_{i1}x_{i} + b_{1})$$
- hidden layer 1 - node2 = $$f(\sum_{i=1}^{3} w_{i2}x_{i} + b_{2})$$
- hidden layer 1 - node3 = $$f(\sum_{i=1}^{3} w_{i3}x_{i} + b_{3})$$
- hidden layer 1 - node4 = $$f(\sum_{i=1}^{3} w_{i4}x_{i} + b_{4})$$

여기서 각 node 마다 bias($b_{1},...b_{4}$)라고 불리는 모수 역시 더해 주어서 활성함수에 넣어 줍니다. 최종적인 딥러닝의 목적은 데이터를 잘 설명하는 weight를 찾는 것입니다. 그리고 이를 위해서는 Backpropagation과 Gradient descent라는 알고리즘을 사용합니다.

## 활성함수(Activation function)

활성함수(Activation function)은 가중치와 전 node의 값을 곱한 것을 전체 합 한 후에 통과시켜 주는 함수입니다. 사실 활성함수를 통과시켜 주기 전까지는 어떻게 해도 값들의 선형 결합이기 때문에 결국 선형 모델 밖에 만들 수 없습니다. 하지만 활성 함수로 비선형성을 부여하게 된다면 비선형 모델로 Neural Net를 사용할 수 있습니다. 선형 모델들이 풀 수 없는 문제 역시 해결할 수 있습니다. 물론 모델을 분석하기에는 더 complex해 집니다. 딥러닝 발달 초기에는 주로 시그모이드 함수를 활성함수로 사용하였지만 요즘은 Relu 함수를 활성함수로 주로 사용합니다. 하지만 어떤 활성함수가 특별히 우월하다고는 할 수 없으므로 여러 개를 써보는 시도가 필요합니다. 자세한 내용을 Neural Net을 트레이닝 시키는 포스팅에서 다루겠습니다.
