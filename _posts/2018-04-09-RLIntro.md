---
title: Reinforcement Learning 입문
category: Reinforcement Learning
tag: Reinforcement Learning
---

강화학습은 딥러닝 분야 뿐만 아니라 다른 분야에서 폭넓게 사용되는 용어입니다.

* 과거의 경험을 통한 학습

* 칭찬이나 꾸중 등을 통해 우리들은 행동거지를 배운다.

### Reinforcement Learning 구성

세팅은 크게 두 가지 *Environment* 와 *Actor* 로 구성됩니다. 우리가 할 일은 Actor가 어떤 Action을 취할지 정하는 일 입니다.

Actor를 Agent라고 부르며 Agent가 Action을 취하면 Environment는 Agent에게 state와 reward를 돌려줍니다. $t$시점에서 상태와 액션을 각각 $s_{t}, a_{t}$라고 하면 다음 시점 $t+1$에서의 상태와 액션을 다음과 같이 나타낼 수 있습니다.

> $s_{(t+1)}, r_{(t)} = env(a_{(t)}, s_{(t)})$

### Q-Learning

#### Dummy Q-Learning

매 state 그리고 그에 따른 action에 대해서 reward값을 돌려주는 $Q-function$를 정의합니다.

> $ reward = Q(state, action) $

예를 들어 $Q$ 반환값이 $Q(s_{1}, a_{1})=0.5, Q(s_{1}, a_{2})=0.3$ 이라면 $a_{1}$를 선택하게 됩니다.

> $\pi^{*}(s)=argmax_{a}Q(s, a)$를 선택합니다.

$\pi^{*}(s)$를 Optimal policy라고 합니다. 만약 이러한 값들이 모두 구해진다면 우리는 $Q-function$이 알려주는 대로 길을 따라 가면 됩니다. 그렇다면 어떻게 $Q$값을 학습할까요? $Q-Learning$이 이를 해결해 줍니다.

상태가 $s$이고 그에 따른 액션이 $a$일 때 그것으로 인한 reward를 $r$, 다음 단계의 상태와 액션 집합을 각각 $s', a'$라고 한다면 $Q(s,a)$의 값 즉, 해당 상황에서의 총 reward는 다음과 같이 표현할 수 있습니다.

> $Q(s, a) \leftarrow r + \max_{a'}Q(s',a')$

구체적으로 $Q(s, a)$ 테이블을 learning하는 단계는 다음과 같습니다.

1. $\hat{Q}(s,a) \leftarrow 0$ : 초기값들 설정
2. 현재 상태를 $s$라고 하자.
3. 무한 반복
* $a$를 시행한다.
* reward $r$를 받는다.
* 새로운 상태 $s'$이 된다.
* 업데이트 $\hat{Q}(s, a) \leftarrow r + \max_{a'}\hat{Q}(s',a')$
* $s \leftarrow s'$

#### Exploit & Exploration Algorithm

- Exploit : 현재 값을 이용
- Exploration : 모험

새로운 길을 찾기 위해서는 모험(Exploration)을 할 필요가 있다. Dummy Q-Learning에서 Q값만 따라가다 보면 모험을 할 수가 없게 됩니다. 이를 보완하기 위해 *E-greedy* 라는 방법을 사용합니다.

```
e = 0.1
if random < e:
  a = random
else:
  a = argmax(Q(s,a))
```
이는 일정한 확률로 랜덤하게 행동을 정하는 것을 의미합니다. 하지만 학습을 거듭할 수록 이를 반영해서 모험 여부를 정해야 합니다. 횟수를 거듭할 수록 랜덤 행동을 줄입니다. 이를 *decaying E-greedy* 라고 합니다.

```
for i in range(1000)
  e = 0.1/(i+1)
  if random(1) < e:
    a = random
  else:
    a = argmax(Q(s,a))
```

문제를 해결하기 위해서 사용하는 또 다른 방법은 random noise를 더해주는 것 입니다. 이를 표현하면:

> $a = argmax(Q(s,a)+random_values)$

이런식으로 한다면 모험을 함으로써 새로운 길을 계속해서 찾을 수 있습니다. 그렇다면 수많은 경로 중에 경로를 결정할 수 있는 방법은 무엇일까요? *discounted reward* 로 이를 해결합니다. 위에서 우리는 업데이트 방법으로 아래와 같은 수식을 얻었습니다.

> $Q(s, a) \leftarrow r + \max_{a'}Q(s',a')$

오른쪽 부분은 미래의 상이므로 현재 리워드($r$)보다 더 작은 가중치를 주는 것이 합리적입니다. 따라서 $discount factor = \gamma \in [0,1]$라고 한다면

> $Q(s, a) \leftarrow r + \gamma\max_{a'}Q(s',a')$

위와 같은 수식을 통해서 $Q(s,a)$를 업데이트 합니다. 이에 함의된 직관적 생각은 시간을 많이 들이며 얻는 보상보다 시간을 적게 들이며 얻는 보상이 더 크다는 것입니다.($\gamma$가 적게 곱해지므로) 따라서 시간을 적게 들이며 Goal로 가는 경로를 찾을 수 있게 됩니다.


#### Convergence

그렇다면 이렇게 구한 $\hat{Q}$은 $Q$로 수렴하는지 궁금할 수 있습니다. 만약,

- deterministic worlds
- finite states

라면 $\hat{Q} \to Q$로 수렴하게 됩니다.

#### Q-Learning in Stochastic(nondeterministic)

* Stochastic model은 내재된 랜덤성을 가지고 있습니다: 똑같은 모수설정과 초기값으로 시작하더라도 서로 다른 결과가 발생할 수 있다.

이와 같은 상황에서는 Q-Learning이 제대로 작동하지 않습니다. 왜냐하면 최적의 선택을 하여도 명령대로 움직이지 않기 때문입니다. 이를 해결하기 위해서 $Q-learning$의 값을 이용하긴 하지만 그 값이 미치는 영향을 제한시키고 원래의 값을 지킬 필요가 있다. 즉, 원래의 $\hat{Q}$와 업데이트되는 $\hat{Q}$값의 가중평균합을 하면 됩니다.

> $\hat{Q}(s,a) \leftarrow (1-\alpha)\hat{Q}(s,a)+\alpha[r+\gamma max_{a'}\hat{Q}(s',a')], \alpha \in [0,1]$

따라서 최종적인 $Q-learning$ 알고리즘은 다음과 같습니다.

1. $\hat{Q}(s,a) \leftarrow 0$ : 초기값들 설정
2. 현재 상태를 $s$라고 하자.
3. 무한 반복
* $a$를 고르고 시행한다.
* reward $r$를 받는다.
* 새로운 상태 $s'$이 된다.
* 업데이트 $\hat{Q}(s,a) \leftarrow (1-\alpha)\hat{Q}(s,a)+\alpha[r+\gamma max_{a'}\hat{Q}(s',a')], \alpha \in [0,1]$
* $s \leftarrow s'$

#### Convergence

여기서 구해지는 $\hat{Q}$ 역시 $Q$로 수렴합니다.

### Q-Network

Q-Table은 현실에 적용하기에 용량 문제가 발생한다. 예를 들어 $100 \times 100$ 미로를 생각하고 액션이 상하좌우라면 테이블 사이즈로 $100 \times 100 \times 4$ 가 필요합니다. 이와 같은 문제 때문에 테이블을 사용하지 않고 네트워크를 사용해서 $Q$값을 학습하는 방법을 사용합니다.

<center><a href="https://imgur.com/5Xt8ZTc"><img src="https://i.imgur.com/5Xt8ZTc.png" width="500px" height="300px" title="source: imgur.com" /></a></center>
(출처: 모두를 위한 강화학습 chapter-6)

뉴럴 네트워크에서 가중치를 $W$라고 할 때 뉴럴 네트워크의 결과물은 $Ws$입니다. 이를 $\hat{Q}(s,a|\theta)$라고 할 수 있습니다. ($\theta$는 뉴럴넷의 모수) 우리는 이 결과를 $Q^{* }(s,a)$로 근사시키고자 합니다. 이는 다음과 같은 Loss function을 최소화하는 $\theta$를 선택하는 것 입니다:

>$\min_{\theta}\sum_{t=0}^{T}[\hat{Q}(s_{t},a_{t}|\theta)-(r_{t}+\gamma \max_{a'}\hat{Q}(s_{t+1},a'|\theta)]^{2}$

즉, $\hat{Q}(s,a|\theta)$ 와 $Q^{* }(s,a)$의 차이의 제곱을 최소화하는 것이 목적입니다. 이와 같은 과정을 *Deep Q-learning* 이라고 합니다.



그런데 왜 알고리즘에서 타겟으로 Stochastic 한 업데이트를 주지 않을까요? 이는 조금씩 업데이트되는 형식이기에 Stochastic 과정과 비슷하게 업데이트 될 수 있어서 타겟을 저렇게 잡는 것은 문제가 되지 않습니다.

#### Convergence

- 샘플들 사이의 Correlation
- Non-stationary target

두 가지 문제로 Diverge되는 경우가 있습니다. 이를 해결하기 위해서 Deepmind에서 DQN이라는 알고리즘을 만들어 냈습니다.

#### DQN(Deep Q-Network)

DQN은 Deepmind에서 만든 위와 같은 문제를 해결하기 위한 방법입니다.(<a src="https://www.nature.com/articles/nature14236.pdf">원논문 링크</a>)

1. 샘플들 사이의 Correlation 문제는 agent가 한 번 액션을 취한다고 해도 환경이 크게 달라지지 않기 때문에 샘플들 사이가 sequential하게 연관되는 문제가 있다는 것 입니다. 예를 들어 주사위 굴려서 칸을 이동하는 게임에서 주사위를 한 번 굴린다면 두 상태의 차이는 별로 나지 않게 됩니다.

* 문제 해결을 위해서 DQN에서는 각 단계별로 (상태, 액션, 보상, 다음 상태)인 $(\phi_{t}, a_{t}, r_{t}, \phi_{t+1})$를 바로 사용하는 것이 아니라 $\mathbb{D}$에 저장해 놓고 random sampling을 통해서 minibatch를 만들어 training에 사용하는 것 입니다. 랜덤하게 샘플링한다면 바로 옆 상태를 고르지 않으니 고른 샘플들 사이의 연관성이 낮아지게 됩니다.

2. Non-stationary target

<center><a href="https://imgur.com/WcZr9xM"><img src="https://i.imgur.com/WcZr9xM.png" width="500px" height="300px" title="source: imgur.com" /></a></center>
(출처: 모두를 위한 강화학습 chapter-7)

기존에 Q-Network에서는 업데이트 할 때 위와 같은 식으로 Loss를 계산하였습니다. 업데이트의 목적은 $\hat{Y} \approx {Y}$하는 것인데, 둘 다 $\theta$를 모수로 하므로 만약 $\theta$가 업데이트된다면 그와 동시에 $Y$ 역시 이동해 버렸습니다. 화살을 쏨과 동시에 과녁이 움직여 버린 것 입니다.

* 문제의 해결은 네트워크를 일단 분리하는 것으로 하였습니다.

<center><a href="https://imgur.com/HOSUZfj"><img src="https://i.imgur.com/HOSUZfj.png" width="500px" height="400px" title="source: imgur.com" /></a></center>
(출처: 모두를 위한 강화학습 chapter-7)

이 방법은 일단 목표 네트워크를 $\bar{\theta}$를 통해 고정시키고 몇 번의 훈련을 통해서 $\theta$를 업데이트 시키는 것입니다. 이전과 달리 $\bar{\theta}$는 고정된 값이기 때문에 타겟인 $Y$가 이동하지 않습니다. 몇 번의 업데이트를 거친 후에 $\theta$ 값을 반영해줍니다. 다음을 알고리즘의 psudocode입니다.

<center><a href="https://imgur.com/lCdKa3w"><img src="https://i.imgur.com/lCdKa3w.png"  width="500px" height="400px" title="source: imgur.com" /></a></center>

(출처:Human-level control through deep reinforcement
learning)
