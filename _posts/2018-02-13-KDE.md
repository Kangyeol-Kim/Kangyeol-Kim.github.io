---
title: Kernel Density Estimation
category: Statistical Methodology
tag: Non-parametric
---

이번 글에서는 데이터의 분포를 non-parametric하게 구해내는 Kernel Density Estimation에 대해 다루겠습니다. Machine Learning에서 많이 쓰이는 방법인 만큼 한 번쯤 정리해 두면 좋을 거 같습니다. 참고자료는 wikipedia와 고려대학교 허명회 교수님의 '탐색적 데이터 분석' 수업 시간의 강의 자료입니다.

## 내용

우리는 흔히 자료에 대한 히스토그램을 그려서 자료들의 분포를 추정합니다. 예를 들어 R 내장 데이터 중에 MASS::geyser$duration를 가지고 분포를 보려고 합니다.

```r
library(MASS)
data(geyser)
attach(geyser)

par(mfrow=c(1, 2))
windows(width = 12, height = 8)
plot(density(duration))
hist(duration, freq = FALSE, xlim=c(0,6))
lines(density(duration), lty=2)
```

<a href="https://imgur.com/WnY4A23"><img src="https://i.imgur.com/WnY4A23.png" width='600px' height='400px' title="source: imgur.com" /></a>

당연히 우리는 직관적으로 이 분포가 맞다고 생각하고 쓰지만 정확히 이 분포는 Kernel Density Estimation 방법에 의한 결과물 입니다. 데이터 $$x_{1}, \cdots x_{n} \sim f(x)$$라고 할 때 $$f(x)$$에 대한 추정.

>$$\hat{f}(x)=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{b}K \Big(\frac{x-x_{i}}{b} \Big)$$

여기서 $$K(z)>0$$ 를 kernel이라고 하고, $$b>0$$ 를 bandwidth라고 부릅니다. 일반적으로 $$K(z)=\frac{1}{\sqrt{2\pi}}e^{-\frac{z^{2}}{2}}$$이고 $$b=0.9 min(s, IQR/1.35)n^{-\frac{1}{5}}$$으로 많이 사용합니다. 즉, kernel은 표준정규분포를 사용하고 그에 따른 표준편차는 저렇게 사용한다고 합니다.(솔직하게 표준편차가 왜 저렇게 되는 지 찾아보지는 않았습니다. 다만, 저것이 연구의 결과라고 합니다.) 우선 $$K(z)$$를 표준정규분포라고 하고 합쳐지는 부분을 보면:

> $$ \frac{1}{b}K \Big(\frac{x-x_{i}}{b} \Big) = \frac{1}{\sqrt{2\pi b^{2}}}e^{-\frac{1}{2}(\frac{x-x_{i}}{b})} \equiv N(x_{i}, b) $$

따라서 각 데이터의 값을 평균으로 하고 표준편차로 $$b$$를 취하는 정규분포임을 알 수 있습니다. 그렇게 $$n$$개의 정규분포를 그리고 난 후에 모든 정의역 구간에서 확률 밀도 값을 전부 더한 후에 다시 $$n$$으로 나누어 주면 됩니다. 일단 $$n$$으로 나누어 주는 것은 우리는 모든 구간에서 적분하면 합이 1이 되는 확률 밀도 함수를 구하고 있는 것이기 때문에 그를 위해서 나누어 주는 것 입니다. 그리고 이렇게 추정값을 구한다면 데이터가 많이 뭉쳐있는 곳은 당연히 높은 확률 밀도 값들이 더해져서  최종 추정 확률 밀도 함수가 높은 값을 가지게 됩니다. 또한, 데이터가 별로 없는 구간에서는 주로 낮은 확률 밀도 값들이 더해지기 때문에 최종 추정 확률 밀도 함수가 낮은 값을 가지게 됩니다. 위에서 언급한 같은 자료로 이를 시각화 할 수 있습니다.

```r
x <- seq(0,7,0.05)
plot(dnorm(x, duration[1],0.33) ~ x, xlim=c(0,7),
     ylim=c(0,2), type="l", xlab="duration",
     ylab="density", main="Geyser duration time")
for (i in 2:299){
  par(new=T)
  plot(dnorm(x, duration[i],0.33) ~ x, xlim=c(0,7),
       ylim=c(0,2), type="l", xlab="", ylab="", main="")
}
summing <- 0
for (i in 1:299) summing <- summing + dnorm(x, duration[i],0.33)
plot(summing/299~x, type='l', lty=2, col='blue', xlab = 'duration', ylab = 'density', main = 'Manual density and base::density ')
par(new=TRUE)
lines(density(duration), lty=2, col='red')
```

<a href="https://imgur.com/sEyHV38"><img src="https://i.imgur.com/sEyHV38.png" width='600px' height='400px' title="source: imgur.com" /></a>

왼쪽 plot은 각 데이터마다 정규 분포를 겹쳐그렸습니다. 이제 이 밀도 함수들을 다 더해서 나누면 오른쪽 같은 그림이 됩니다. 실제로 density 함수를 겹쳐 그려봐도 거의
완전히 일치하는 것을 알 수 있습니다.

한편 이변량 자료의 경우 위의 경우와 매우 비슷하게 확률 밀도 함수를 추정합니다. ($$(x_{1},y_{1}),...,(x_{n}, y_{n}) \sim f(x,y)$$)

>$$\hat{f}(x)=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{b_{1}}K \Big(\frac{x-x_{i}}{b_{1}} \Big) \frac{1}{b_{2}}K \Big(\frac{x-x_{i}}{b_{2}} \Big)$$

```r
x <- waiting
y <- duration
symbols(waiting[1], duration[1], circles=1,
        inches=0.1, xlim=c(40,100), ylim=c(0,6),
        xlab="waiting", ylab="duration",
        main="Geyser")
for (i in 2:299){
  par(new=T)
  symbols(waiting[i], duration[i], circles=1,
          inches=0.1, xlim=c(40,100), ylim=c(0,6),
          xlab="", ylab="", main="Geyser")
}

density.1 <- kde2d(waiting, duration, n=100)
image(density.1, xlab = "waiting", ylab = "duration")
```

<a href="https://imgur.com/Mqatqh4"><img src="https://i.imgur.com/Mqatqh4.png" width='600px' height='400px' title="source: imgur.com" /></a>

## Kernel에 대해

여기서는 표준정규분포를 kernel로 사용했지만 다른 분포를 kernel로 사용해도 됩니다. 예컨대 연속균등분포나 정규분포와 비슷한 모양을 하고 있는 거라면 말이죠. 정규분포가 kernel로 쓰는 의미는 데이터가 자기 자신에게 가장 높은 값을 부여하고 자신과의 거리에 반비례하게 값을 주려는 생각인 것 같습니다.
