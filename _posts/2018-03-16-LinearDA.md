---
title: Linear Discriminant Analysis / Quadratic Discriminant Analysis
category: Basic Machine Learning Algorithm
tag: Classification
---

Discriminant Analysis는 종속변수가 범주형 일때 설명변수들을 사용해서 효과적으로 그룹들을 분류하는 데 목적을 둡니다. 이 포스팅에서는 그 방법으로써 LDA/QDA 를 다루겠습니다. LDA는 두 가지 단계를 거칩니다. 첫 번째는 dimension reduction하는 단계 그리고 그 다음으로 Classification을 행합니다.

## Multivariate Normal Distribution(MVN)

$x=(x_{1},x_{2},...,x_{p}), x_{1},...,x_{p}=random \ variables$라고 한다면 MVN 분포는 두 가지 모수를 가집니다:

* 평균 $\mu$, $p$차원 벡터
* 공분산 행렬 $\Sigma$, $p \times p$ 차원

벡터 $x$가 MVN 분포를 따르는 것을 $x \sim N(\mu, \Sigma)$ 혹은 $x \sim MVN(\mu, \Sigma)$라고 표기하고 다음과 같은 밀도 함수를 가집니다.

>$f(x \lvert \mu, \Sigma) = \frac{1}{(2\pi)^{p/2} \lvert \Sigma \lvert^{1/2}} \exp[-\frac{1}{2}(x-\mu)^{t}\Sigma^{-1}(x-\mu)], \lvert \Sigma \lvert=determinant \ of \ \Sigma$

MVN의 평균은 그래프의 중심점을, 공분산 행렬은 퍼진 정도를 의미합니다. 예를 들어 $x \sim MVN[\binom{0}{0}, \binom{1 \ .8}{.8 \ 3}]$이라면 그에 따른 그래프는 아래와 같이 그려집니다.

<center><a href="https://imgur.com/NHS2fnl"><img src="https://i.imgur.com/NHS2fnl.png" width="400px" height="400px" title="source: imgur.com" /></a></center>

LDA에서는 자료들이 모두 같은 공분산을 따른다고 가정하고 이론을 전개합니다.

## Dimension reduction via LDA

LDA는 데이터가 방향 $a$로 projection했을 때, 데이터를 가장 잘 분류하는 방향 $a \in R^{p}$를 찾는 것 입니다. 각 관측치 $x_{i} \in R^{p}$에 대해 방향 $a$로의 projection은 $t_{i} = a^{t}x_{i} \in R$로 구할 수 있고 이를 *discriminant scores* 라고 부릅니다. 그렇다면 잘 분류하는 것은 무엇일까요? 그룹별 $t_{i}$의 평균이 서로 멀리 떨어져 있고 그룹 내에서는 $t_{i}$값들의 분산이 작으면 됩니다. 즉, $F=\frac{Between \ group \ variation}{Within \ group \ variation}$를 최대화시키면 되는 것 입니다.

<center><a href="https://imgur.com/YrQXL1y"><img src="https://i.imgur.com/YrQXL1y.png" width="500px" height="400px" title="source: imgur.com" /></a></center>

방향 $a$를 찾는 아이디어는 그룹간의 평균 score의 차이를 더한 것을 분자로 하고 그룹내의 score의 분산들의 합을 분자로 하는 것이었습니다. 그룹 내와 그룹 사이의 표본 공분산 행렬은:

$C_{W}=\sum_{k=1}^{K}\sum_{l-1}^{n_{k}}(x_{k,l}-\bar{x_{k}})(x_{k,l}-\bar{x_{k}})^{t}$

$C_{B}=\sum_{k=1}^{K}n_{k}(\bar{x_{k}}-\bar{x})(\bar{x_{k}}-\bar{x})^{t}$

수식으로만 보면 이해하기 어려우니 그림을 통해 이를 볼 수 있습니다. 처음에 주어진 데이터는 3개의 클래스 데이터라고 한다면 각 평균과 전체 평균을 구할 수 있습니다.

<center><a href="https://imgur.com/b2GLWiy"><img src="https://i.imgur.com/b2GLWiy.png" width="400px" height="400px" title="source: imgur.com" /></a></center>

그리고 이를 통해서 $C_{B}, C_{W}$를 구할 수 있습니다. 그림에서는 $S_{B}, S_{W}$에 해당합니다.

<center><a href="https://imgur.com/YoKEd36"><img src="https://i.imgur.com/YoKEd36.png" width="600px" height="300px" title="source: imgur.com" /></a></center>

그러면 Fisher의 비율은 $F=\frac{a^{t}C_{B}a}{a^{t}C_{W}a}$과 같이 구해집니다. 그냥은 이 문제를 풀수는 없으니까 $a^{t}C_{W}a=1$이라는 조건 아래서 이 비율을 최대화시키려고 합니다.

> $\max_{a}a^{t}C_{B}a \ subject \ to \ a^{t}C_{W}a=1$

이 제약 조건에서 라그랑주 승수법에 따라 $L(a, \lambda) = a^{t}C_{B}a - \lambda(a^{t}C_{W}a-1)$이를 $a, \lambda$에 대해서 미분해서 0으로 놓아서 풀 수 있습니다. $C_{B}, C_{W}$는 대칭 행렬이므로  

$$
\begin{align*}
  \frac{\partial L(a, \lambda)}{\partial a}&=a^{t}(C_{B}+C_{B})-a^{t}(C_{W}+C_{W})=0\\
  &\Leftrightarrow a^{t}C_{B}=\lambda^{t}C_{W}\\
  &\Leftrightarrow C_{B} a=\lambda C_{W} a\\
  &\Leftrightarrow C_{W}^{-1}C_{B}a=\lambda a
\end{align*}
$$

이를 통해 $a$에 대한 답은 행렬 $C_{W}^{-1}C_{B}$의 eigenvector임을 알 수 있습니다. PCA와 유사하게 LDA에서도 적절한 차원의 $k$를 선택하여 자료의 차원을 감소시킵니다.

<center><a href="https://imgur.com/KwaYVqZ"><img src="https://i.imgur.com/KwaYVqZ.png" width="400px" height="400px" title="source: imgur.com" /></a></center>

이러한 projection 과정을 그림으로 나타내면 다음과 같습니다.

<center><a href="https://imgur.com/PAxEADr"><img src="https://i.imgur.com/PAxEADr.png" width="300px" height="300px" title="source: imgur.com" /></a></center>

## Classification via LDA & QDA

LDA와 QDA 모두 모수적인 통계 방법입니다. 그룹이 $K$개 존재한다고 하고 새로운 관측치인 $x_{new}$가 주어졌을 때 그룹으로 분류하기 위해서 각 $k$에 대해서 $P(y_{new}=k \lvert x_{new})$를 계산하면 됩니다. 베이즈 정리에 의해서 이 식은 다음과 같이 구할 수 있습니다.

$P(y_{new}=k \lvert x_{new})=\frac{P(y_{new}=k)f(x_{new} \lvert y_{new}=k)}{\sum_{l=1}^{K}P(y_{new}=l)f(x_{new} \lvert y_{new}=l)}$

Discriminant Analysis에서는 $k$ 그룹에 속하는 관측치들이 $MVN(\mu_{k}, \Sigma_{k})$에 속한다고 가정합니다. 이에 따라서 $f(x_{new} \lvert y_{new}=l)$ 부분을 계산할 수 있습니다:

>$f(x \lvert y=k)=f(x \lvert \mu_{k}, \Sigma_{k}) = \frac{1}{(2\pi)^{p/2}\lvert \Sigma_{k} \lvert^{1/2} \exp [-\frac{1}{2}(x-\mu_{k})^{t}\Sigma_{k}^{-1}(x-\mu_{k})]}$

또한 $P(y_{new}=k)$($\pi_{k}$로 표기)을 계산하기 위해서 이전 데이터들의 그룹별 관측치 수를 이용합니다. 즉, $n$이 전체 관측치이고 $n_{k}$가 $k$그룹의 관측치 일 때 $\pi_{k}=P(y_{new}=k)=\frac{n_{k}}{n}$ 이제 각 $k$ 별로 확률을 계산할 수 있습니다.

그런데 각각의 확률 계산에서 분모 부분이 같으므로 $P(y=k \lvert x) \propto \pi_{k}f(x \lvert \mu_{k}, \Sigma_{k})$ 인 관계가 성립합니다. 따라서 두 그룹 사이의 분류를 생각하고 $x$가 그룹 $k$로 분류되는 경우는

>$P(y=k \lvert x) > P(y=l \lvert x) \Leftrightarrow \pi_{k}f(x \lvert \mu_{k}, \Sigma_{k}) > \pi_{l}f(x \lvert \mu_{l}, \Sigma_{l})$

위의 계산식을 대입한 후에 로그를 취하면:

> $\log\pi_{k}-\frac{1}{2}\log \lvert \Sigma_{k} \lvert - \frac{1}{2}(x-\mu_{k})^{t}\Sigma_{k}^{-1}(x-\mu_{k}) > \log\pi_{l}-\frac{1}{2}\log \lvert \Sigma_{l} \lvert - \frac{1}{2}(x-\mu_{l})^{t}\Sigma_{l}^{-1}(x-\mu_{l})$

이와 같은 방법으로 분류하는 것을 *Quadratic Discriminant Analysis (QDA)* 라고 합니다. 위와 같은 계산에서 만약 모든 공분산 행렬이 같은 경우인 $\Sigma_{k} = \Sigma$를 생각하면 QDA 공식을 단순화 시킬 수 있습니다.

$\log\pi_{k}-\frac{1}{2}\log \lvert \Sigma \lvert - \frac{1}{2}(x-\mu_{k})^{t}\Sigma^{-1}(x-\mu_{k}) > \log\pi_{l}-\frac{1}{2}\log \lvert \Sigma \lvert - \frac{1}{2}(x-\mu_{l})^{t}\Sigma^{-1}(x-\mu_{l})$

$\Leftrightarrow \log\pi_{k}+\mu_{k}^{t}\Sigma^{-1}x - \frac{1}{2}\mu_{k}^{t}\Sigma^{-1}\mu_{k} > \log\pi_{l}+\mu_{l}^{t}\Sigma^{-1}x - \frac{1}{2}\mu_{l}^{t}\Sigma^{-1}\mu_{l}$

$\Leftrightarrow (\mu_{k}-\mu_{l})^{t} \Sigma^{-1} x > \log \frac{\pi_{l}}{\pi_{k}} + \frac{1}{2}(\mu_{k}+\mu_{l})^{t}\Sigma^{-1}(\mu_{k}-\mu_{l})$

$\Leftrightarrow a^{t}x > c$

마지막 부분을 보면 선형 분류임을 알 수 있습니다. 이러한 방법을 Linear Discriminant Analysis(LDA)를 통해서 분류를 하는 것 입니다.

### LDA/QDA에 관한 몇 가지 설명

* $P(data \lvert group)$이 MVN분포를 따르는 것을 가정합니다.
* 이러한 가정은 각 그룹의 데이터들이 타원형 모양(elliptical shape)를 따르는 것을 가정하는 것을 의미합니다.
* LDA는 그룹들이 같은 공분산 행렬을 가지는 것을 허용하지만 QDA는 그렇지 않습니다.
* QDA의 경우 데이터를 $k$개의 그룹으로 나누어서 공분산 행렬을 추정합니다. $x_{k,l},l=1,...,n_{k}$를 $k$번째 그룹의 $l$번째 데이터라고 한다면 공분산 행렬은 다음과 같이 구해집니다.

>$\Sigma_{k}=\frac{1}{n_{k}-1}\sum_{l=1}^{n_{k}}(x_{k,l}-\bar{x_{k}})(x_{k,l}-\bar{x_{k}})^{t}$, $\bar{x_{k}}$는 그룹 $k$의 평균

* LDA의 경우에는 $\Sigma_{1}=\Sigma_{2}= \cdots =\Sigma_{K}$를 가정하기 때문에 *pooled* 공분산 행렬을 구해야 합니다. 이는 $\Sigma_{k}$에 가중치 $n_{k}$로 주고 구해집니다.

>$\Sigma_{pool}=\frac{(n_{1}-1)\Sigma_{1}+(n_{2}-1)\Sigma_{2}+\cdots+(n_{K}-1)\Sigma_{K}}{(n_{1}-1)+(n_{2}-1)+\cdots+(n_{K}-1)}=\frac{1}{n-K}\sum_{k=1}^{K}\sum_{l=1}^{n_{k}}(x_{k,l}-\bar{x_{k}})(x_{k,l}-\bar{x_{k}})^{t}$

이렇게 구한 $\Sigma_{pool}$를 LDA에서 $\Sigma$로 사용합니다.
