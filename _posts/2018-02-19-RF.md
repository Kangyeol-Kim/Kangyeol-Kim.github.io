---
title: DT를 이용한 Bagging = Random Forest
category: Basic Machine Learning Algorithm
tag: Ensemble
---

Machine Learning 모델들은 많은 현실적인 문제들을 훌륭하게 해결하고 있지만, 그렇지 않은 분야도 무수히 많이 존재합니다. Machine Learning 모델들이 흔히 마주하는 문제는 모델이 overfitting 되거나 underfitting되는 문제입니다. 이 중 **Random Forest** 알고리즘은 decision tree의 알고리즘이 가지고 있는 overfitting 문제를 해결하기 위해서 **decision tree 모델들로 변형된 Bagging** 하는 것을 의미합니다. 깊이가 매우 깊은 decision tree 모델들은 일반적으로 낮은 Bias와 매우 높은 Variance를 가지고 있습니다. 따라서 이러한 모델들로 Bagging하는 것은 매우 효과적인 결과를 얻을 수 있습니다. 이 글에서는 Random Forest 알고리즘이 어떻게 돌아가는지 살펴보고 그 특성을 몇 개 살핀 후에 python으로 구현해 보겠습니다.

## Random Forest 방법 설명

위에서 Random Forest(이하 RF)는 decision tree(DT) 모델들로 **변형된** Bagging을 하는 것이라고 하였습니다. <a href="https://kangyeol-kim.github.io/basic%20machine%20learning%20algorithm/2018/02/19/esb/">앙상블 기법</a>에서 언급했듯이 Bagging은 training data에서 복원추출로 몇 번의 sample을 뽑아서 각각에 대해서 머신 러닝 모델을 만드는 것입니다. 하지만 RF에서는 변수도 Random하게 추출하여 training data에 대한 부분집합을 만듭니다. 구체적인 알고리즘은 다음과 같이 작동합니다.

0.    INPUT(training data($$X,Y$$), proportion of data sample($$p$$), number of random features($$k$$), number of trees($$B$$))
1.    $$b=1,...,B$$에 대해서 $$D$$로 부터 $$p$$퍼센트의 데이터를 **복원추출** 하고 random하게 뽑은 특성 $$k$$개를 이용하여 데이터 $$X_{b}, Y_{b}$$을 만듭니다.
2.    $$X_{b}, Y_{b}$$를 통해서 DT 모델 $$f_{b}$$를 만듭니다.
3.    새로운 데이터 $$\tilde{x}$$에 대해서 Regression은 $$\hat{f}=\frac{1}{B} \sum_{b=1}^{B} f_{b}(\tilde{x})$$ 혹은 Classification의 경우 투표를 통해서 $$\hat{f}$$를 결정합니다.

앞서 언급했다시피 이렇게 하면 DT 모델들의 높은 Variance를 낮출 수 있습니다. 이렇게 합쳐진 모델의 분산은 다음과 같이 줄어 듭니다:

> $$\sigma = \sqrt{\frac{\sum_{b=1}^{B} (f_{b}(\tilde{x})-\hat{f})^{2}}{B-1}}$$

근데 왜 굳이 Bagging 기법과 다르게 변수들도 뽑아서 데이터를 구성하는 것일까요? 이는 DT 알고리즘의 특징때문입니다. DT에서는 information gain 혹은 gini impurity를 통해서 최적의 변수들을 뽑아 냅니다. 그런데 항상 모든 변수들을 가지고 최적의 변수를 찾아내면 $$f_{b}$$들이 대부분 같은 변수를 가지고 만들어 질 것 입니다. 그러면 모델을 합치는 의미가 별로 없습니다. 따라서 변수들 역시 뽑아서 최대한 많은 변수를 모델링 하는 데 사용하려는 의도입니다.

## $$p, k, B$$ 값의 결정에 대해서

0번에서 INPUT으로 3가지 모수를 정해 주었습니다. proportion of data sample($$p$$)은 뽑는 데이터 비율인데, 보통 Regression은 $$p/3$$의 내림값, Classification의 경우 $$\sqrt{p}$$의 내림값을 사용하는 것으로 알려져 있습니다. 한편, $$p=1$$로 하는 경우 그러니까 데이터는 항상 전체 개수를 복원 추출 하되 변수는 random하게 선택하는 경우가 있는데, 이같은 경우는 중복되지 않은 데이터가 63.2%가 뽑히는 것으로 기대됩니다. $$k$$의 경우 적당한 값을 결정해서 사용합니다. RF에서 주로 최적화하는 모수는 tree의 개수(B) 입니다. 이는 cross-validation을 통해서 구해집니다.

## Python code

python에서 복수의 DT를 만들어서

```python
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.datasets import load_breast_cancer
import numpy as np
import pandas as pd
from sklearn import metrics

# Data loading

cancer = load_breast_cancer()
data = pd.concat([pd.DataFrame(cancer.data, columns= cancer.feature_names), pd.DataFrame(cancer.target, columns=["is_cancer"])], axis=1)
test, train = train_test_split(dat, test_size = 0.7, random_state = 123)

# Manual function for random forest define

def RF_for_BinaryData(train, test, nTrees, nFeatures, sampleProp):
    #########################################################
    ##   nTrees = number of trees                          ##
    ##   nFeatures = number of feature for each sampling   ##
    ##   sampleProp = proportion of data for each sampling ##
    #########################################################
    N = train.shape[0]
    F = train.shape[1]
    preds = []

    for i in range(nTrees):
        N_idx = np.random.choice(N, size=int(N*p), replace=True)
        F_idx = np.random.choice(F-1, size=nFeatures, replace=True)

        subX = train.iloc[N_idx, F_idx]
        subY = train.iloc[N_idx, F-1]

        tree=DecisionTreeClassifier()
        tree.fit(X = subX, y = subY)
        pred = tree.predict(X = test.iloc[:,F_idx])
        preds.append(pred)

    vote = np.sum(preds, axis = 0)
    vote = (vote > nEstimators/2).astype(int)
    return metrics.classification_report(vote, test.iloc[:,F-1])

# implementation

print(RF_for_BinaryData(train = train, test = test, nTrees = 10, nFeatures = 6, sampleProp = 0.7))
```

<center><a href="https://imgur.com/mRiUByx"><img src="https://i.imgur.com/mRiUByx.png" width="500px" height="500px" title="source: imgur.com" /></a></center>
