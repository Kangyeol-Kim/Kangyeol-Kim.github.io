---
title: K-Nearest Neighborhood
category: Basic Machine Learning Algorithm
tag: Classification
tag: Regression
---

**K-Nearest Neighborhood(KNN)** 은 Supervised Learning으로 Classification 혹은 Regression 문제를 해결하는 기본적인 Machine Learning 알고리즘입니다. 이 글에서는 KNN으로 Classification 혹은 Regression을 어떻게 해결하는지 또한 벡터 사이의 거리 역시 고려한 weighted KNN에 대해서 다루겠습니다.

## 설명

### 방법

KNN 기존 데이터 중 가장 유사한(둘 사이의 거리가 짧은) k개의 이웃 데이터들을 이용해서 새로운 데이터를 예측하는 방법입니다. 새로운 데이터($$(\tilde{x})$$)에 대해서 Classification의 경우 k개의 데이터에서 majority voting을 해서 $$\tilde{y}$$를 결정하고, Regression은 k개의 데이터 평균을 통해서 $$\tilde{y}$$를 결정합니다. 우리가 결정해야할 Hyperparameter는 k의 수와 데이터들 사이의 거리를 어떻게 측정할 것인지(Distance measure) 입니다.


### Distance measure

두 벡터 $$\textbf{x}=(x_{1},...,x_{p}), \textbf{y}=(y_{1},...,y_{p})$$가 있다고 할 때, 둘 사이의 거리를 측정할 수 있는 다양한 방식이 있다.

1. Euclidean distance : $$d(x,y)=(\sum_{i=1}^{p} (x_{i}-y_{i})^{2})^{\frac{1}{2}} $$
2. Manhattan distance : $$d(x,y)=\sum_{i=1}^{p} \lvert x_{i}-y_{i} \rvert$$
3. Minkowski distance : $$d(x,y)=(\sum_{i=1}^{p} (x_{i}-y_{i})^{m})^{\frac{1}{m}}$$
4. cosine similarity : $$\frac{\sum_{i=1}^{n} x_{i} \times y_{i}}{\sqrt{\sum_{i=1}^{n} x_{i}^{2}} \times \sqrt{\sum_{i=1}^{n} y_{i}^{2}}}$$

4 코사인 유사도는 거리(얼마나 먼지)가 아니라 유사도(얼마나 가까운지)에 대한 측정이기 때문에 높을 수록 거리가 가깝다고 생각해야 합니다. 나머지는 거리이기 때문에 값이 낮을 수록 가깝다고 생각해야 합니다.

### k 값 결정

underfitting과 overfitting을 피하기 위해서는 적절한 k 값을 선택해야 합니다. 이에 대해서는 보통 cross validation이라는 방법을 사용합니다. 데이터가 주어지면 일단 train data와 test data로 나눕니다. 그리고 train data를 다시  n 등분해서 (n-1)개의 데이터로 모델을 만들고 나머지 데이터로 특정 Hyperparameter 상황에서 모델에 대한 테스트를 합니다. 그러면 이 과정이 끝나면 n개의 모델과 각 모델을 평가한 결과 n개가 있겠죠. 결과 n개를 평균해서 지정한 Hyperparameter 아래서의 성능이라고 합니다. Hyperparameter를 바꾸어 가면서 이 과정을 반복한다면 Hyperparameter1-성능, Hyperparameter2-성능, Hyperparameter3-성능,... 가 얻을 수 있습니다. 이 중 가능 좋은 성능을 가진 Hyperparameter를 가지고 최종적인 모델을 만들면 됩니다. 예컨대 여기서는 k=3-성능, k=5-성능, k=7-성능,...를 구한 후에 가장 좋은 k를 선택하면 되는 거죠.


## 다른 고려해야 할 점

### Feature scaling

변수마다 측정 단위가 다르므로 모두 같이 더하여 거리를 구하는 방법으로는 거리를 제대로 재기가 어렵습니다. 따라서 변수들의 단위를 맞추어 주는 과정이 필요합니다. 크게 두 가지 방법으로  scaling 할 수 있습니다. $$\textbf{X}=(x_{1}, ..., x_{n})$$에 대해

1. min-max normalization : $$X_{s}=\frac{X-min(X)}{max(X)-min(X)}$$
2. z-score normalization : $$X_{s}=\frac{X-\mu}{\sigma}$$

### Weighted KNN

k개의 가까운 데이터를 가지고 새로운 데이터에 대한 값을 정하긴 하지만 k개의 데이터의 거리가 모두 동일하지 않기 때문에 유사도(=1/거리)에 비례해서 k개의 데이터마다 가중치를 부여하는 방법을 생각할 수 있다.

|클래스 | 거리 | 유사도 | 가중치 |
|A |1 |1 | 0.44|
|B |2 |0.5 | 0.22|
|C |3 |0.33 | 0.15|
|B |4 |0.25 | 0.11|

이에 따라 원래라면 B인 데이터가 더 많아 B로 분류해야 하지만, 가중치를 준다면 A의 가중치=0.44 > B의 가중치 = 0.33 이므로 A로 분류하게 된다.
