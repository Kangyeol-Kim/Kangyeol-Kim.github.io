---
title: Deep Learning 입문
category: Deep Learning
tag: Deep Learning
---

## 딥러닝의 발달

딥러닝은 약 60년 전부터 제기된 개념이지만 몇 번의 침체기를 거쳐서 오늘날에 주목 받는 개념이 되었습니다. 요즘 다시 딥러닝이 부흥하는 이유로는 여러 학자들의 이론적 breakthrough를 이루어 낸 것과 컴퓨팅 파워가 현격하게 증가했다는 점. 그리고 데이터의 양이 폭발적으로 증가하여 딥러닝의 핵심을 이루는 학습(Learning)이 전보다 더 잘 이루어질 수 있다는 점을 꼽을 수 있습니다.

<center><a href="https://imgur.com/TsuQ0Vx"><img src="https://i.imgur.com/TsuQ0Vx.jpg" width="600px" height="400px" title="source: imgur.com" /></a></center>

딥러닝이 기존 머신러닝 알고리즘과 다른 점은 데이터의 변수를 사람이 직접 뽑아낼 필요가 없다는 점입니다. 즉, **feature engineering** 이 필요하지 않다는 점입니다. 기존의 머신러닝 모델들은 데이터의 차원이 지나치게 높아질 수록 성능이 좋지 않았습니다. 따라서 지나친 고차원 데이터를 피하기 위해서 데이터를 잘 설명하는 변수를 선택할 필요가 있었습니다. 이러한 작업은 여러 방법론을 통해서 사람의 힘으로 하였는데, 딥러닝 모델들은 이와 달리 모델 자체에서 변수 선택을 합니다.

## Neural Net & Perceptron

Neural Network은 머신 러닝 알고리즘 중 하나인 ANN(Artificial Neural Net)를 보완 발전시켜서 만든 구조입니다. Neural Net은 인간의 뇌 구조 중 뉴런(Neuron)을 본떠서 만든 퍼셉트론(Perceptron)이라는 단위로 구성됩니다.

<center><a href="https://imgur.com/1AvWf1e"><img src="https://i.imgur.com/1AvWf1e.png" width="400px" height="400px" title="source: imgur.com" /></a></center>

왼편 그림에서 알 수 있듯이 인간의 뉴런은 여러 개의 자극을 받아 결과를 냅니다. 인공적인 퍼셉트론 역시 INPUT으로 무언가를 받아서 OUTPUT를 냅니다. 좀 더 구체적으로는 INPUT으로 들어올 때 가중치(weight)가 곱해지고 이를 모두 더한 후에 activation function이라고 불리는 함수를 통과하여 결과를 내게 됩니다. 퍼셉트론 여러개를 이용하면 Neural Net 구조를 만들 수 있습니다. 다음은 아주 간단한 Neural Net 구조입니다.

<center><a href="https://imgur.com/R6MxuaL"><img src="https://i.imgur.com/R6MxuaL.png" width="400px" height="400px" title="source: imgur.com" /></a></center>

Neural Network가 만들어지면 위와 같은 구조를 가지게 됩니다: *input layer-hidden layer-output layer*. 이 떄 hidden layer 개수를 마음대로 정할 수 있는 데, 이를 계속해서 늘려나갈 수 있습니다.  이를 deep 해진다고 표현합니다. Deep Neural Network는 여기서 나온 말 입니다. 이 구조를 요약하면 다음과 같습니다:

- input layer node 수(=데이터 변수 개수) : 3
- hidden layer 개수 : 2
- hidden layer node 수 : 4, 4
- output node 수 : 1

input layer node 수는 데이터에서 변수의 개수를 통해서 결정됩니다. 각 변수의 값이 node안으로 들어가게 되는 것 입니다.  또한 output node 는 데이터의 타겟 변수를 의미합니다. 즉, 위와 같은 Neural Net 구조는 $f_{NN}$를 해당 모델의 함수라고 한다면 $y = f_{NN}(x_{1}, x_{2}, x_{3})$와 같습니다. 한편 각 layer를 연결하는 화살표인 weight에 대해서 살펴봅시다. 각 layer들의 node와 node 간에는 반드시 하나의 weight가 존재합니다. 따라서 input layer - hidden layer 1를 연결하는 weight는 $$3 \times 4 = 12$$개가 존재하고, hidden layer 1 - hidden layer 2를 연결하는 weight는 $$4 \times 4 = 16$$개가 존재합니다. 간단하게 input layer - hidden layer 1 사이를 계산하여 hidden layer 1의 node 값들을 계산해 봅시다.$x_{1}, x_{2}, x_{3}$를 세 개의 변수값이라고 하고 $$w_{ij}, \ i=1,2,3; \ j=1,2,3,4$$를 input layer 하나의 node와 hidden layer 1 하나의 node 사이의 weight라고 하고 $$f$$를 활성함수라고 한다면

- hidden layer 1 - node1 = $$f(\sum_{i=1}^{3} w_{i1}x_{i} + b_{1})$$
- hidden layer 1 - node2 = $$f(\sum_{i=1}^{3} w_{i2}x_{i} + b_{2})$$
- hidden layer 1 - node3 = $$f(\sum_{i=1}^{3} w_{i3}x_{i} + b_{3})$$
- hidden layer 1 - node4 = $$f(\sum_{i=1}^{3} w_{i4}x_{i} + b_{4})$$

여기서 각 node 마다 bias($b_{1},...b_{4}$)라고 불리는 모수 역시 더해 주어서 활성함수에 넣어 줍니다. 최종적인 딥러닝의 목적은 데이터를 잘 설명하는 weight를 찾는 것입니다. 그리고 이를 위해서는 Gradient descent라는 알고리즘을 사용합니다.

## 활성함수(Activation function)

활성함수(Activation function)은 가중치와 전 node의 값을 곱한 것을 전체 합 한 후에 통과시켜 주는 함수입니다. 사실 활성함수를 통과시켜 주기 전까지는 어떻게 해도 값들의 선형 결합이기 때문에 결국 선형 모델 밖에 만들 수 없습니다. 하지만 활성 함수로 비선형성을 부여하게 된다면 비선형 모델로 Neural Net를 사용할 수 있습니다. 선형 모델들이 풀 수 없는 문제 역시 해결할 수 있습니다. 물론 모델을 분석하기에는 더 complex해 집니다. 딥러닝 발달 초기에는 주로 시그모이드 함수를 활성함수로 사용하였지만 요즘은 Relu 함수를 활성함수로 주로 사용합니다. 하지만 어떤 활성함수가 특별히 우월하다고는 할 수 없으므로 여러 개를 써보는 시도가 필요합니다.

## 얼마나 깊어야 하나?

Deep 이라는 용어가 함의하는 것은 위에서 언급한 구조에서 hidden layer의 개수가 늘어나면서 신경망 층이 깊어진다는 의미입니다. 이 말은 곧 모델의 모수를 늘린다는 것을 의미하고 복잡한 문제를 해결할 수 있는 가능성을 늘려줍니다. 실제로 *ImageNet* 이라는 이미지 분류 문제를 푸는 
-깊은게 좋냐?
parameter가 많으면 모델이 복잡해지고 더 많은 분류를 해낼 수 있다.
어느정도까지?

## Backpropagation


## Softmax

-output layer에서 softmax로 확률화
==> 이걸로 loss로 구함
-log(x) 함수
-log(0.13)=0.89 loss 값 그래프보면 1일때 loss 0 0일 떄 loss 무한대로 감


**Update 중입니다**
