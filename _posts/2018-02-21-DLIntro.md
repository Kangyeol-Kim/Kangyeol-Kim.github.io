---
title: Deep Learning 입문
category: Deep Learning
tag: Deep Learning
---

## 딥러닝의 발달

딥러닝은 약 60년 전부터 제기된 개념이지만 몇 번의 침체기를 거쳐서 오늘날에 주목 받는 개념이 되었습니다. 요즘 다시 딥러닝이 부흥하는 이유로는 여러 학자들의 이론적 breakthrough를 이루어 낸 것과 컴퓨팅 파워가 현격하게 증가했다는 점. 그리고 데이터의 양이 폭발적으로 증가하여 딥러닝의 핵심을 이루는 학습(Learning)이 전보다 더 잘 이루어질 수 있다는 점을 꼽을 수 있습니다.

<center><a href="https://imgur.com/TsuQ0Vx"><img src="https://i.imgur.com/TsuQ0Vx.jpg" width="600px" height="400px" title="source: imgur.com" /></a></center>

딥러닝이 기존 머신러닝 알고리즘과 다른 점은 데이터의 변수를 사람이 직접 뽑아낼 필요가 없다는 점입니다. 즉, **feature engineering** 이 필요하지 않다는 점입니다. 기존의 머신러닝 모델들은 데이터의 차원이 지나치게 높아질 수록 성능이 좋지 않았습니다. 따라서 지나친 고차원 데이터를 피하기 위해서 데이터를 잘 설명하는 변수를 선택할 필요가 있었습니다. 이러한 작업은 여러 방법론을 통해서 사람의 힘으로 하였는데, 딥러닝 모델들은 이와 달리 모델 자체에서 변수 선택을 합니다.

## 얼마나 깊어야 하나 혹은 깊은게 무조건 좋냐?

Deep 이라는 용어가 함의하는 것은 신경망 층이 깊어진다는 의미입니다. 이 말은 곧 모델의 모수를 늘린다는 것을 의미하고 복잡한 문제를 해결할 수 있는 가능성을 늘려줍니다. 실제로 *ImageNet* 이라는 이미지 분류 문제를 푸는 대회에서는 지속적으로 신경망 층이 깊게 하면서 모델의 성능을 끌어올리는데 성공하는 모습을 보여주었습니다. 하지만 모델의 층을 늘려서 더욱더 복잡한 모델을 만들어 쓰는 것이 장점만 가지는 것은 아닙니다. 다음은 몇 가지 문제의 예시입니다.

* 모수 개수의 폭발적인 증가로 학습을 위한 컴퓨팅 파워 증가
* 어느 정도까지 복잡하게 모델을 만들어야 할 것 인가?
* 꼭 층을 쌓는 것이 좋으냐? 층의 node를 늘려도 되지 않을까?

먼저 모델을 깊게 만들기 위해서는 그에 상응하는 컴퓨팅 파워가 필요합니다. 또한 복잡성의 정도 문제 역시 있습니다. 만약 모수의 개수를 많이 줄이고도 그와 비슷한 결과를 얻을 수 있다면 굳이 복잡한 모델을 구성할 필요가 없습니다. 마지막으로 가로로 모델을 비대하게 하지말고 세로로 각 층의 node 개수를 늘리는 방향으로 모델 구성을 하면 어떻게 될까? 라는 문제가 있습니다. 딥러닝이 현재 활발하게 연구되고 빠르게 변화하는 만큼 이에 대한 연구가 계속 되다보면 어느정도 답을 얻을 수 있지 않을까 개인적으로 생각합니다.

## Hyperparameter

자동으로 학습되는 parameter와 달리 Hyperparameter는 사람이 정해주는 것으로 이를 정하기 위해서는 모델을 만들고 이를 테스트하는 과정을 통해서 정하게 됩니다. 데이터가 그리 크지 않은 경우에는 Cross-validation을 통해서 Hyperparameter를 정합니다. 데이터를 k개의 부분으로 나누어서 데이터 각각을 train data - validation data - test data로 나누는 것 입니다.

<center><a href="https://imgur.com/ToDObDT"><img src="https://i.imgur.com/ToDObDT.png" width="600px" "350px"  title="source: imgur.com" /></a></center>

위의 그림과 같이 데이터를 나누어서 모델을 여러 개 만들어서 테스트 합니다. 이는 validation 셋이 랜덤하게 정해지기 때문에 Hyperparameter 결정시 여러 개의 모델을 만들어서 더 정확하게 하기 위해서 입니다.

## Loss function

딥러닝은 결국 최적의 가중치 $W$를 구하는 문제입니다. 이를 위해서는 $W$의 성능을 구할 지표가 필요합니다. 이러한 역할은 Loss function을 통해서 가능합니다. Loss function은 현재의 분류기가 얼마나 좋은지 측정해 줍니다. 데이터를 $\{(x_{i}, y_{i})\}, i=1,2,...,N$이라고 하고 L_{i}를 Loss funciton에 의한 값, L를 Loss라고 한다면 다음의 관계가 성립합니다:

>$L(W) = \frac{1}{N}\sum_{i}L_{i}(f(x_{i}, W), y_{i})$

이는 예측값과 실제값의 차이를 Loss function인 $L_{i}$로 구하는 것을 의미합니다.

## Regularization

>$L(W) = \frac{1}{N}\sum_{i}L_{i}(f(x_{i}, W), y_{i})$

과 같은 방법으로 $W$를 구한다면 training data에 아주 적합한 $W$를 구할 수 있지만, 그것이 test data 역시 잘 맞출지는 장담할 수 없습니다. 만약 지나치게 training data에 적합하게 $W$를 찾는다면 즉, 모델을 만든다면 오히려 test data는 잘 맞추지 못할 수도 있습니다. 이를 *Overfitting* 문제라고 합니다. 모델이 지나치게 over되었다는 뜻입니다. 이를 방지하기 위해서 Regularization term($\lambda R(W)$)을 도입합니다.

>$L(W) = \frac{1}{N}\sum_{i}L_{i}(f(x_{i}, W), y_{i})+\lambda R(W)$

$R(W)$는 만약 $W$가 지나치게 커지거나 작아지면 페널티를 부여해서 지나친 $W$추정을 하지 못하게 합니다. $\lambda$는 얼마나 모델이 복잡해지는 것을 막을 것인지 정해주는 모수입니다. 이는 $\lambda$를 크게 하는 것은 모델이 복잡해 질수록 페널티를 크게 준다는 사실을 의미합니다. 하지만 $\lambda$가 지나치게 커지면 모델을 복잡하게 만들지 못해 training data도 어느 정도 적합시키지 못하는 모델이 만들어 집니다. 이러한 문제를 *Underfitting* 이라고 합니다. 따라서 우리는 적당한 $\lambda$ 값을 선택해야 합니다. 한편 $R(W)$의 후보에는 다음과 같은 것들이 있습니다.

* L2 Regularization : $R(W)=\sum_{k}\sum_{l}W_{k,l}^{2}$

* L1 Regularization : $R(W)=\sum_{k}\sum_{l}\lvert W_{k,l} \lvert$

* Elastic net(L1+L2) : $R(W)=\sum_{k}\sum_{l}\beta W_{k,l}^{2}+\lvert W_{k,l} \lvert$

* Max norm regularization

* Dropout

## Softmax

딥러닝으로 분류 문제를 해결할 때 마지막 부분에서는 각 class별 점수를 softmax 함수를 통해서 확률화시켜줍니다. 모든 점수를 다음과 같이 확률화 시킵니다.

> $P(Y=k \lvert X=x_{i})=\frac{e^{s_{k}}}{\sum_{j}e^{s_{j}}}, s_{j} = \ score \ of \ j \ class$

이 데이터의 정답은 $k$라고 한다면 우리는 $k$일 확률을 최대한 높히려는데 목적이 있습니다. 만일 확률화 시킨 후에 Loss funciton으로 정답의 확률에 $-\log$를 취해 준다면:

> $L_{i}=-\log(\frac{e^{s_{k}}}{\sum_{j}e^{s_{j}}})$

이런식으로 softmax loss를 정의하면 두 가지 이점을 얻을 수 있습니다. 일단 $-\log$함수의 그래프 개형을 본다면 아래와 같습니다.

<center><a href="https://imgur.com/Ok1Guqv"><img src="https://i.imgur.com/Ok1Guqv.png" width="500px" height="350px" title="source: imgur.com" /></a></center>

* 확률이 0로 가면 loss가 무한대로 커진다.
* 확률이 1로 가면 loss가 0으로 수렴한다.

만약 정답의 확률이 0에 가깝다면 loss가 클 것입니다. 반대로 확률이 1에 가깝다면 loss가 거의 0일 것입니다. 이는 매우 합리적인 loss 설정이라고 할 수 있습니다.

## Optimization

loss를 정의했다면 loss가 최소화되는 지점을 찾는 게 우리의 목적입니다. 마치 산골짜기에서 산등성이를 타고 계속 내려가는 것처럼 최소화되는 지점을 찾습니다. 이러한 *Gradient Descent* 알고리즘으로 시행합니다. gradient는 다차원 공간에서의 미분으로 벡터값을 갖습니다. gradient는 함수의 증가율이 가장 큰 방향을 가르키며 그 값은 그때의 방향의 기울기의 정도를 나타냅니다. 따라서 $W$에 의존한 loss function의 gradient를 구하고 이를 빼주면 loss의 감소율이 가장 크도록 $W$를 이동시킬 수 있습니다. 그 과정에서 얼마나 이동시킬 건지를 learning rate라고 불리는 $\alpha$로 결정하게 됩니다. 이와 같은 과정을 반복해서 loss의 최소화 지점을 구하게 됩니다.

<center><a href="https://imgur.com/WJaYCJ3"><img src="https://i.imgur.com/WJaYCJ3.png" width="400px" height="400px" title="source: imgur.com" /></a></center>

한편, 딥러닝에서는 데이터 사이즈 $N$이 매우 크기 때문에 모든 데이터에 대해 loss를 계산하는 것은 계산량이 매우 많이 드는 작업입니다. 따라서 데이터를 분리하여 (각 부분을 minibatch라고 부릅니다.) 각 부분에서 gradient를 계산하고 업데이트하고 다음 부분 gradient를 계산하고 업데이트하는 식으로 최적화를 하게 됩니다. 이를 *Stochastic Gradient Descent* 라고 부르며 현실에서는 이런 식의 train 과정을 대부분 사용하고 있습니다.
