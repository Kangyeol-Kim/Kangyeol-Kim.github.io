---
title: Deep Learning 입문
category: Deep Learning
tag: Deep Learning
---

## Intro & 기본 구조

Neural Network 구조는 머신 러닝 알고리즘 중 하나인 ANN(Artificial Neural Net)를 보완 발전시켜서 만든 구조입니다. 이 구조는 인간의 뇌 구조 중 뉴런(Neuron)을 본떠서 만들었습니다.

<<그림1>>

왼편 그림에서 알 수 있듯이 인간의 뉴런은 여러 개의 자극을 받아 결과를 냅니다. 인공적인 Neural Network 구조 역시 INPUT으로 무언가를 받아서 OUTPUT를 냅니다. 좀 더 구체적으로는 INPUT으로 들어올 때 가중치(weight)가 곱해지고 이를 모두 더한 후에 activation function이라고 불리는 함수를 통과하여 결과를 내게 됩니다. 다음은 아주 간단한 Neural Net 구조입니다.

<<그림2>>

Neural Network가 만들어지면 위와 같은 구조를 가지게 됩니다: input layer-hidden layer-output layer, 이 떄 hidden layer 개수를 마음대로 정할 수 있는 데, 이를 계속해서 늘려나갈 수 있습니다.  이를 deep 해진다고 표현합니다. Deep Neural Network가 여기서 나온 말 입니다. 이 구조를 요약하면 다음과 같습니다:

- input layer node 수(=데이터 변수 개수) : 3
- hidden layer 개수 : 2
- hidden layer node 수 : 4, 4
- output node 수 : 1

input layer node 수는 데이터에서 변수의 개수를 통해서 결정됩니다. 또한 output node 는 데이터의 타겟 변수를 의미합니다. 한편 각 layer를 연결하는 화살표인 weight에 대해서 살펴봅시다. 각 layer들의 node와 node 간에는 반드시 하나의 weight가 존재합니다. 따라서 input layer - hidden layer 1를 연결하는 weight는 $$3 \times 4 = 12$$개가 존재하고, hidden layer 1 - hidden layer 2를 연결하는 weight는 $$4 \times 4 = 16$$개가 존재합니다. 간단하게 input layer - hidden layer 1 사이를 계산하여 hidden layer 1의 node 값들을 계산해 봅시다. $$w_{ij}, \ i=1,2,3; \ j=1,2,3,4$$를 input layer 하나의 node와 hidden layer 1 하나의 node 사이의 weight라고 하고 $$f$$를 activation function이라고 합시다.

- hidden layer 1 - node1 = $$f(\sum_{i=1}^{3} w_{i1}x_{i} + b_{1})$$
- hidden layer 1 - node2 = $$f(\sum_{i=1}^{3} w_{i2}x_{i} + b_{2})$$
- hidden layer 1 - node3 = $$f(\sum_{i=1}^{3} w_{i3}x_{i} + b_{3})$$
- hidden layer 1 - node4 = $$f(\sum_{i=1}^{3} w_{i4}x_{i} + b_{4})$$

여기서 각 node 마다 bias라고 불리는 모수가 있는 것을 알 수 있습니다.

## Activation function

Activation function은 가중치와 전 node의 값을 곱한 것을 전체 합 한 후에 통과시켜 주는 함수입니다. 사실 Activation function을 통과시켜 주기 전까지는 어떻게 해도 값들의 선형 결합이기 때문에 결국 선형 모델 밖에 만들 수 없습니다. 하지만 Activation function으로 비선형성을 부여하게 된다면 비선형 모델로 Neural Net를 사용하게 됩니다. 이렇게 모델을 더 complex하게 만들 수 있습니다. 딥러닝 발달 초기에는 시그모이드 함수를


-깊은게 좋냐?
parameter가 많으면 모델이 복잡해지고 더 많은 분류를 해낼 수 있다.
어느정도까지?

-parameter를 많이 하기 위해서 넓게 하면 어떠냐?

연구 중





<그림>

-모델을 학습시킨다는 것은 weight들을 학습시키는 것이다.

-퍼셉트론 ==> Multilayer Perceptron / Fully Connected Layer

-딥러닝이 발달한 이유;

-다양한 활용이 있다.

-Neural network에 대해서


-Activation function을 왜쓰냐 : 비선형성을 주기 위해서 더 complex한 모델을 만들기 위해서

-마지막 결과값은 결국 선형 결합, 하지만 데이터 공간이 변화한다.
 Bias 왜? 분류선이 왔다갔다 할 수 있다.

-깊은게 좋냐?
parameter가 많으면 모델이 복잡해지고 더 많은 분류를 해낼 수 있다.
어느정도까지?

-parameter를 많이 하기 위해서 넓게 하면 어떠냐?

연구 중

-output layer에서 softmax로 확률화
==> 이걸로 loss로 구함
-log(x) 함수
-log(0.13)=0.89 loss 값 그래프보면 1일때 loss 0 0일 떄 loss 무한대로 감
