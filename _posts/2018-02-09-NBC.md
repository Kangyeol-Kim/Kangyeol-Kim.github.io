---
title: Naive Bayes Classifier
category: Basic Machine Learning Algorithm
tag: Classification
---

나이브 베이즈 분류기(이하 NBC)는 지도 학습(Supervised Learning) 환경에서 작동하는 Classifier 중 하나입니다. 이번 글에서는 NBC가 어떤 구조로 되어있는지 구체적으로 살펴보고 그에 필요한 몇 가지 통계적 개념에 대해서 살펴보겠습니다. (1) 조건부 확률&베이즈 정리, 조건부 독립, MLE / (2) NBC 모델의 작동 원리 / (3) 그 외 다른 것

# 쓰이는 통계적 개념

### 조건부 확률&베이즈 정리

사건 A,B가 있을 때, **B가 일어 났을 때 A의 확률** 을 $$P(A \lvert B)$$라고 표기합며 이를 사건 B가 일어났을 때 사건 A의 조건부 확률이라고 부릅니다. 한편, 조건부 확률은 **베이즈 정리** 에 의해 다르게 풀어 쓸 수도 있습니다.

>$$ P(A|B)=\frac{P(A,B)}{P(B)}=\frac{P(B|A)P(A)}{P(B)} $$

중요한 것은 사건 B가 일어났을 때 사건 A의 조건부 확률을 사건 A가 일어났을 때 사건 B의 조건부 확률로 나타낼 수 있다는 것 입니다.

### 조건부 독립

사건 F,G가 H가 주어진 상황에서 조건부 독립이라는 말은 H라는 상황이 주어진다면 F,G는 서로 상관없는 독립적인 사건이라는 말입니다. 수식으로 나타내면 아래와 같이 됩니다:

$$P(F \cap G|H)=P(F|H)P(G|H)$$

두 사건이 상관없으니 확률이 따로따로 곱한 것과 같이 됩니다.

### Maximum Likelihood(MLE)

MLE는 데이터들을  통해 데이터들의 모수(parameter)를 구하는 방법입니다. 어떤 모수(parameter)가 주어졌을 때, 원하는 값들이 나올 Likelihood 를 최대로 만드는 모수를 선택하는 방법입이다. 갑자기 어려운 말들이 쏟아져 나왔는데, 여기서는 데이터들이(\{x_{1},...,x_{n}\}) 각각 독립적으로 특정한 분포($$f(\cdot|\theta)$$)에서 sampling 됬다고 생각합니다. 그리고 $$n$$개의 데이터가 동시에 관찰된 확률 분포를 Likelihood라고 부르며 이를 $$L(\theta)=\prod_{i=1}^{n} f(x_{i}|\theta)$$라고 정의합니다. MLE 방식은 이러한 Likelihood가 최대로 만드는 $$\theta$$값을 찾는 것 입니다. 아래서 다시 한 번 언급하겠지만, NBC 모델에서는 연속형 변수가 정규 분포(normal distribution)을 따르고 범주형 변수들은 다항 분포(Multinomial distribution)을 따른다고 생각합니다. 그리고 MLE 방식 가지고 데이터들을 통해서 분포들의 모수를 구합니다.

# NBC 모델의 작동 원리

주어진 데이터가 $$(x_{1},...,x_{p})$$와 같은 형태이고, 그에 따른 분류가 $$(C_{1},...C_{m})$$으로 된다고 합시다. NBC의 목적은 $$(x_{1},...,x_{p})$$가 주어졌을 때 $$C_{k}(1 \leq k \leq m)$$의 조건부 확률들을 구해서 그 중 확률값이 maximize하는 $$C_{k}$$로 분류를 하는 것입니다.

### 모델 구조

다시 말해, $$m$$개의 $$P(C_{k}|x_{1},...,x_{p})$$를 계산해서 이를 비교하면 됩니다. 먼저 계산해 봅시다.

$$
\begin{align*}
  P(C_{k}|x_{1},...,x_{p})&=\frac{P(x_{1},...,x_{p}|C_{k})P(C_{k})}{P(x_{1},...,x_{p})} \\
  &\propto P(x_{1},...,x_{p}|C_{k})P(C_{k}) \\
  &= P(x_{1}|C_{k}) \cdots P(x_{p}|C_{k})P(C_{k})
\end{align*}
$$
